{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36afeb4f",
   "metadata": {},
   "source": [
    "# LING4181 Supervised Reading\n",
    "# Spring 2023\n",
    "# Final Essay\n",
    "# Candidate Number: 1102\n",
    "# 1 June 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fefc4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Authorship attribution is an age old problem that relates to the study of (writing) style where style is a unique variety of language use in an individual. (Crystal, 2008) The previous sentence implies that there is a unique set of stylistic markers that an author habitually or unconsciously employs in their writing, and that style is something that can be attributed back to an individual. In real-life applications, however, it is hard to simply say that, since multiple questions are usually intertwined to form one problem: for example, how many candidates are there, how long are the documents in question, and how old are they? <br>\n",
    "Two famous authorship attribution cases show the variability of questions in application: Shakespeare’s authorship problem and the Unabomber case. The two cases show a similarity in that there are multiple lengthy documents to assign, but they are different in that the primary objective of the former is to verify whether the documents share the same author, or whether the proposed candidate (William Shakespeare, or someone else) is the correct author, while that for the latter is finding a certain individual(s) that is likely to have written the document, among an open pool of candidates. What features we consider more important than others can drastically change the approach to the solution, too. Are some features more reflective of the author than others? Is it realistically possible to assign a correct author to an unattributed text with them?<br>\n",
    "With this essay, I aim to attempt to answer such questions, tying together the readings to get an overview of the matter at hand and reporting the procedures and results from my own experiments based on the methods mentioned in the readings. In the grand scheme of things, the semester was divided into two parts that virtually progressed at the same time: first, finding and reading previous studies, and second, replicating methods from the readings, including learning to code in Python and collecting data to test them on. <br>\n",
    "There were clear limitations to this endeavor: to name a few, I could not verify the reliability of each method using valid statistical methods due to lack of time, and I did not replicate methods employing machine learning techniques. Discussions and reflections on the procedures will follow in more detail in the final section. Still, the semester as well as the replication attempts were a good opportunity to investigate a topic, which could potentially lead to more in-depth studies in the future: I plan to continue the research, as this small project left more questions than there were before.<br>\n",
    "The report is structured as the following: I first piece together several previous studies related to the topic and explain central issues of authorship. Then I explain the methods I replicated in more detail with actual codes I used. Finally, I present the results and evaluate the successfulness and limitations of the project.<br>\n",
    "\n",
    "\n",
    "## Background\n",
    "### Problems and paradigms in stylometry\n",
    "Authorship attribution, in the sense it has in the present essay, is a quantitative approach to the study of stylistics, and the goal of authorship attribution is to assign correct author(s) to the unattributed text. In automated authorship attribution tasks, “text” refers to written documents and this does not include texts produced and conveyed using other media such as pictures, audio, or video. Stylistics could be understood as an intersection of literary analysis and linguistics, but it is widely applicable in a variety of other contexts as well, such as historical/religious studies, authorship credits/plagiarism-related controversies, political discourse analysis, and criminal investigations (including online attacks and feuds).<br>\n",
    "Different situations call for different approaches, as Koppel et al. (2013) classify typical attribution problems like the following (the names in parentheses are taken from Koppel et al. (2013, p. 318)):<br>\n",
    "1. Problems where the goal is to find a most likely author among a closed set of candidate authors and an abundance of sample text is available (“simple authorship attribution”),\n",
    "2. problems where the goal is to identify if two long texts are written by the same author (“long-text verification”): in other words, it is a binary question, and\n",
    "3. problems where the goal is to attribute a text to one author among a large set of candidates (“many-candidates problem”).\n",
    "\n",
    "\n",
    "In addition to this, there are problems where there are very many candidates among which we do not even know if the true author is included, or the candidates’ samples or the unattributed texts are too short for comparison. Koppel et al. poses “the fundamental problem” which is a problem of determining whether two (shorter) texts were written by the same person or two different authors. \n",
    "There are two main paradigms in authorship attribution that is considered, which can (and should) be modified depending on the type of task at hand: similarity-based approach on one hand and machine-learning methods on the other. A similarity-based method relies on the similarity between the query text (the anonymous text to be attributed) and a known author’s style profile – all available writing by that author is considered like a single document. “Similarity” can be defined in a variety of ways. A machine-learning method use writings of a candidate author as training data to construct a classifier that categorizes anonymous documents. (Koppel et al., 2013; Koppel et al., 2012; Koppel & Winter, 2014)<br>\n",
    "\n",
    "### Stylistic features\n",
    "Which linguistic features represent the individual writing better than others is the central question in authorship attribution, especially in similarity-based methods. Here, I introduce two concepts proposed in previous studies with varying reliability: the methods I replicate are presented in more detail in the next section.<br>\n",
    "CUSUM method (Morton, 1991; Morton and Michaelson, 1990), which is short for cumulative sum analysis, considers the occurrence of several variables within a sentence, such as the words starting with a vowel or words that are two or three letters long. The cumulative sum of to what degree these measurements as well as the sentence length deviate from the average of the whole text is calculated and compared to known author profiles that are calculated the same way. The assumption that forms the basis of this method is that language habits are constant, and the two profiles would match each other if the same author wrote both. (Coulthard et al., 2017) This method was accepted in court multiple times from 1991, before its reliability and the validity of its assumptions were refuted (Hardcastle, 1993, 1997; Totty et al., 1987). <br>\n",
    "Zipf’s law (1932) provide us with useful insights about the pattern of how word-types in a (natural language) text are distributed. Zipf found that when we rank all word-types in order of frequency, the frequency of the type at ith rank is inversely proportional of the rank. Put differently, the frequency of the most common word is about twice as much as the second most common word. From this, we can predict that a few word-types cover a majority of tokens in a corpus: just 150 most frequent word-types account for 50-65% of a text. (Savoy, 2020)<br>\n",
    "This inspires using word frequency as an important variable in characterizing a certain individual’s language habit. Various measurement methods were since proposed with relation to word frequency, each with a different view on what words are more meaningful to measure frequency of: type-token ratio is a classic index; Burrows (2002) takes the most frequent word-types, Labbé (2007) considers all word-types, and so on.<br>\n",
    "Several non-linguistic aspects influence one’s writing style, including gender, period in which the text is written, age, and genre. In what form these aspects are reflected in statistically measurable linguistic features, and to what extent those linguistic features are chosen consciously, and therefore to what extent these stylistic preferences reflect cognitive processes or social backgrounds, are all unclear. The endeavor to find answers to these questions might cross the boundaries of stylistics.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb0ac6",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ea901",
   "metadata": {},
   "source": [
    "In this section, I explain in detail the data collection procedure and methods of analysis. As was mentioned in the previous section, there are various types of tasks in authorship attribution: simple closed-set problem, verification problems with a binary question, many-candidates problems are the common types of stylometry tasks. In real-world authorship attribution problems, however, there are also instances of co-authorship, which requires more complex methods of analysis which will not dealt with in the present paper. <br>\n",
    "The methods employed in the replications illustrated in this section are applicable for closed-set, single-author problems, and the data were collected accordingly. The methods can be divided to two large sections: lexical analysis and distance-based analysis. I collected data and wrote the codes myself, but the formula for each quantitative analysis largely followed chapters 2 and 3 from Savoy(2020).\n",
    "\n",
    "\n",
    "### Data collection\n",
    "Two sets of data consisted with texts from three authors each were collected. I call them `data1` and `data2` respectively.\n",
    "As the writing style of an individual varies across themes (what the text is about) and channels (what platform the text was intended for), I decided one channel and two themes: about gardening and true crime, on blogs run by private individuals. Sample blogs are found by several simple google search sessions using keywords \"gardening\" and \"true crime\". Three random blogs each with enough amount of text were selected. In total, texts from 3 gardening-related blogs were included in the first dataset (`data1`), and three true-crime blogs were included in the other (`data2`). Texts were manually collected going from reverse chronological order. A short excerpt from each author were stored separately as test(query) text. The query text is not included in the sample text. (In this essay, \"text\" refer to each sample text whose author is the same, unless otherwise noted.)\n",
    "\n",
    "Things worth noting about texts and collecting them:\n",
    "1. I do not personally know any of the blog owners that were included in the sample. (Links to the source blogs are included in the bibliography.)\n",
    "2. Author C in data1 was one of three co-writers in the blog, so I filtered out the two other authors and only included one.\n",
    "3. Some manual processing was involved to delete links, advertisement, mentions of the names of the blog owners.\n",
    "\n",
    "### Blog posts as a text\n",
    "I use blog posts as sample texts for the experiment. Blogs, like other channels, show several characteristic features which should be considered when analyzed:\n",
    "1. Blogs are online platforms for individuals or groups of people to share thoughts, experiences, or knowledge on various topics. Blog posts are typically reverse-chronologically organized, though there is no set rule for organizing posts.\n",
    "2. There is no set rules for the type of content, formality or layout for blog posts. Therefore, blog writers have a flexibility of writing styles. Some blogs deal with a single topic seriously, while others have different purposes.\n",
    "3. Many different types of media including images, audio files, and videos can be incorporated into the posts. These can be embedded inside the posts so that the readers can directly check them out without leaving the page, or provided as hyperlinks to external websites. This implies that a lot can be explained without using written words, and there can be mixed-medium contexts where the context cannot be understood entirely if only the text is considered.\n",
    "4. It is a platform for relatively casual writings, and thus blog posts include words and phrases characteristic of casual online texts. At the same time, blogs are usually for long-form contents: language use characteristic of short-form text platforms such as YouTube comment sections and Twitter is rarely present in blog posts. It is difficult to generalize linguistic features specific for blog posts since there is a great variety of forms and themes across blogs, but the blogs included in the sample tended to be casual and long-form.\n",
    "\n",
    "I presumed that inter-genre variability within a single individual would be bigger than interpersonal variability within a single genre, but blogs might be a different story. Blogs operate under self-determined rules, which might lead us to predict that there can be more room for variability between blogs than, say, tweets.\n",
    "\n",
    "Sample texts as well as full codes can be seen on ((LINK)). Codes are attached as appendix in addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218d4a7",
   "metadata": {},
   "source": [
    "**Table 1**<br>\n",
    "Summary of sample data (texts a, b, c belong to `data1`, while d, e, f belong to `data2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dd721",
   "metadata": {},
   "source": [
    "![table1](Table1_type_token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e5af9",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "First, necessary packages are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8006577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e19029",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Each text went through preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d56e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    return text.translate(str.maketrans(\"\",\"\", string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eca73a",
   "metadata": {},
   "source": [
    "As such, all letters were converted to lowercase letters, and was removed punctuation marks (more about this later), then split to word units instead of letter unit strings. In other words, all texts after this are a list of all the words it contains, not a string of letters, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fefac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these', 'weeks', 'just', 'after', 'the', 'calendar', 'turns', 'from', 'one', 'year', 'to', 'the', 'next', 'are', 'the', 'perfect', 'time', 'to', 'think', 'about', 'your', 'goals', 'for', 'the', 'coming', 'gardening', 'season', 'on', 'this', 'week’s', 'podcast', 'i', 'discuss', 'plotting', 'out', 'plans', 'for', 'doubling', 'down', 'on', 'what', 'worked', 'well', 'in', 'the', 'garden', 'while', 'also', 'deciding', 'on']\n"
     ]
    }
   ],
   "source": [
    "text_a = preprocess('author_a.txt')\n",
    "text_b = preprocess('author_b.txt')\n",
    "text_c = preprocess('author_c.txt')\n",
    "text_d = preprocess('author_d.txt')\n",
    "text_e = preprocess('author_e.txt')\n",
    "text_f = preprocess('author_f.txt')\n",
    "print(text_a[0:50]) # processed text looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788064a0",
   "metadata": {},
   "source": [
    "### Lexical Analysis\n",
    "Lexical analysis methods refer to methods that make use of surface lexical information. They all have in common the fact that they utilize some aspect of word frequency in a given corpus(text). Several such methods have been proposed that consider different aspects of the word frequency distribution. <br>\n",
    "\n",
    "#### Lexical Diversity\n",
    "One way of quantitatively evaluating word choice is to measure the diversity of word use - in other words, how many types of words were used in relation to the total length (number of word-tokens) in a text. <br>\n",
    "\n",
    "##### Type-token ratio\n",
    "Type-token ratio is a simple index for measuring lexical diversity in a given text. A low type-token ratio indicates low degree of diversity in the choice of words the author made. It can be simply calculated as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c23b5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def typetoken_ratio(text):\n",
    "    return round(len(set(text))/len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa3bb7",
   "metadata": {},
   "source": [
    "And thus Table 1 is completed as the following: <br><br>\n",
    "**Table 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ce59",
   "metadata": {},
   "source": [
    "![table2](Table2_typetoken_ratio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2c662",
   "metadata": {},
   "source": [
    "The number of all word-types is divided by the number of all tokens to calculate type-token ratio, and more nuanced information such as the frequency of each word-type is not reflected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe2ad",
   "metadata": {},
   "source": [
    "##### Simpson's D\n",
    "Simpson's D (1949) is another indicator of lexical diversity. It is a useful index that is less influenced by the text length. Simpson's D measures vocabulary richness by calculating the sum of probabilities of selecting the same word twice in two separate trials. The formula is as follows (taken from Savoy(2020):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b60ac",
   "metadata": {},
   "source": [
    "**(1)** $$Simpson's  D(T) = \\displaystyle\\sum_{r=1} \\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot \\mid Voc_{r}(T) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583330e4",
   "metadata": {},
   "source": [
    "- T refers to the text.\n",
    "- n refers to the corpus size, i.e. the number of tokens in T.\n",
    "- r refers to the number of times a given word type appears in T.\n",
    "- VOC<sub>r</sub>(T) refers to the number of word types that appear in T exactly r times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fd62",
   "metadata": {},
   "source": [
    "When there is no diversity in the usage of words, in other words there are only one word that is used throughout the entire text ($ r=n $), the formula returns 1, which is the maximum value, Hence, the closer to 0 Simpson's D value is, the richer the vocabulary use is for the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7c7ba",
   "metadata": {},
   "source": [
    "The formula in **(1)** is rewritten as codes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4875737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson_D(text):\n",
    "    count = collections.Counter(text)\n",
    "    types = set(text)\n",
    "    n = len(text)\n",
    "    def VOC(r):\n",
    "        VOC = 0\n",
    "        for i in types: # i is a word(type)\n",
    "            if count.get(i) == r:\n",
    "                VOC += 1\n",
    "        return VOC\n",
    "    if sum(VOC(r) for r in range(1, n)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(sum(VOC(r) * (r**2 - r) / (n**2 - n) for r in range(1,n+1)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261e2d",
   "metadata": {},
   "source": [
    "The function `simpson_D(text)` takes a preprocessed text as its argument. `collections.Counter` function creates a dictionary type data where the key is the word type and the value is its occurence in the text. `VOC(r)` is an inner function that is defined as the number of word types(`i`) in `text` that appears r times. Since it presupposes `i` appears at least once in `text`, `VOC(r)` always appears as a positive number. Hence the absolute value sign in **(1)** is unnecessary. The function, then, calculates the sum of $\\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot Voc_{r}$ for all $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f771c",
   "metadata": {},
   "source": [
    "By  definition, the function should return 1 when $r=n$. However, the code above somehow always returns `0.0`. From a practical point of view, $ r=n $ is unlikely to happen, since we are dealing with a real-life language use where there are more than 1 word type in a text. But for the sake of completing the equation, the following lines were added,\n",
    "```python\n",
    "if sum(VOC(r) for r in range(1, n)) == 0:\n",
    "        return 1\n",
    "```\n",
    "which returns 1 if there is all `r` is zero until $r=n-1$.\n",
    "<br>\n",
    "Simpson's D can be used to compare the query text to candidate texts to find out which candidate text is the closest in terms of lexical diversity. Results will be discussed in section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef016bc",
   "metadata": {},
   "source": [
    "#### Other lexical stylometric measures\n",
    "##### Big Words Index\n",
    "Big Words Index is defined as the percentage of words consisted of 6 letters or more in a given text. A text with high percentage of big words require more cognitive resources to process, effectively meaning more difficult to read. (Savoy, 2020) I used 7 letters as the threshold instead, because I was sceptical of the significance of 6 letter words, since 4-5 character long nouns can take plural form and become 6 character long. The same applies for verbs and a few other parts of speech. <br>\n",
    "The function `BWI` illustrated below takes a text as its argument. The text will have been preprocessed and made into a list type data before being put. If a word in that text is 7 letters or longer, `big_word` is increased by 1, and after going through all items in the list, the function returns the percentage of 'big words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64de7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BWI(text):\n",
    "    big_word = 0\n",
    "    for word in text:\n",
    "        if len(word) >= 7:\n",
    "            big_word += 1\n",
    "    return round(big_word / len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1807",
   "metadata": {},
   "source": [
    "##### Mean sentence length\n",
    "In a similar way to big words, mean sentence length can help us understand the complexity of the text, as longer sentences are more difficult to understand than short ones. Sentence length can also reflect syntactical choices made by the writer; phrase-structuring habits like *'Tom's'* as opposed to *'of Tom'* can influence the length of the sentence. <br>\n",
    "The codes below show how it is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c0c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sent(filename): # put raw text, not split by space or deleted punctuation marks!\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    t = sent_tokenize(text)\n",
    "    split = []\n",
    "    for sent in t:\n",
    "        a = sent.translate(str.maketrans(\"\",\"\", string.punctuation)).split()\n",
    "        split.append(a)\n",
    "    return sum(len(sent) for sent in split) / len(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbbe65",
   "metadata": {},
   "source": [
    "The function `mean_sent` takes a raw, unprocessed text as its argument and preprocesses the text inside itself. I used sentence tokenizer provided in the Natural Language ToolKit(NLTK), which turns the original text into a list of sentences, split by full stops, question marks and other common sentence-ending punctuation marks. Then, each item in the list(each sentence) is split by spaces. At this point, the text is a multi-dimensional list where each item(sentence) is again a list of words. It is then possible to calculate how many items(words) there are inside each item(sentence), and calculating mean sentence length is as simple as dividing the sum of sentence lengths by the length of the text(number of sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892f575",
   "metadata": {},
   "source": [
    "##### Mean word length\n",
    "\n",
    "\n",
    "To make a fairer comparison for mean word length between sample texts, I took the first 15,000 words from each text of `data1`, since the shortest text of data1 (`text_c`) has about 15,000 tokens. The sample texts in `data2` do not have to go through this process, since they all contain more than 50,000 tokens each and there are small differences between the lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a_split = text_a[:15000]\n",
    "text_b_split = text_b[:15000]\n",
    "text_c_split = text_c[:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c2be1",
   "metadata": {},
   "source": [
    "Calculating mean word length is simpler and more precise than calculating mean sentence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3247c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word(text):\n",
    "    return sum(len(word) for word in text) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df4ad",
   "metadata": {},
   "source": [
    "##### Word length distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f94fa",
   "metadata": {},
   "source": [
    "Another way of taking word length into consideration is checking word length distribution. <br>\n",
    "The function `wordlength` presented below shows how many words that are exactly `r` letters long are in a given text. It creates a list `X` of integers from 1 to the maximum word length in the text (`n`). The internal function `length` counts the number of words that consist of exactly `r` letters. Then `X`,which contains all possible word length, is paired with the the values from the internal function `length` and turned into a dictionary-type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlength(text):\n",
    "    dist = {}\n",
    "    n = len(max(text, key=len))\n",
    "    X = list(i for i in range(1,n+1))\n",
    "    def length(r):\n",
    "        length = 0\n",
    "        for word in text:\n",
    "            if len(word) == r:\n",
    "                length += 1\n",
    "        return length\n",
    "    for x in X:\n",
    "        dist[x] = length(x)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b49bd",
   "metadata": {},
   "source": [
    "The dictionary returned at the end, `dist`, has the word length in natural numbers as keys and the number of words as long as that as values. We can then compare it to another text's word length distribution, or visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f5c",
   "metadata": {},
   "source": [
    "##### Lexical Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb1abc",
   "metadata": {},
   "source": [
    "Lexical density (LD) is the ratio between the number of lexical items (1-functional words) and the text length.\n",
    "Functional words (stop words) include frequently used words that carry little meaning but grammatical information. Here, I used a predefined list of stop words provided in NLTK and selected English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14bec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def l_density(text):\n",
    "    filtered = []\n",
    "    for w in text:\n",
    "        if w not in stopwords:\n",
    "            filtered.append(w)\n",
    "    return round(len(filtered) / len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b662cc8",
   "metadata": {},
   "source": [
    "The function `l_density` filters out function words from a text and computes the percentage for the remainder against the number of all tokens in the text. Therefore, the closer to 1 the value is, the 'denser' the text is. Some comparative values are necessary: I follow Savoy(p.30) where it said (and I paraphrase) that an LD value of around 0.3 for an oral production and around 0.4 and higher for writings are the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec5ddc",
   "metadata": {},
   "source": [
    "### Distance-based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8095b3",
   "metadata": {},
   "source": [
    "Distance-based methods establish a profile for each candidate author to which we can compare the query text's profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d3b2",
   "metadata": {},
   "source": [
    "Burrows' Delta (Savoy 2020: 34-36) is one of such methods: it considers 40-150 most frequent word types, and the style is reflected through the word choice. According to Savoy(34), 150 most frequent word types cover 50-65% of all tokens in a certain text, with the percentage varying depending on the theme, genre, etc. of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53522b",
   "metadata": {},
   "source": [
    "The following is the formula for Delta (taken from Savoy(p.37)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31066087",
   "metadata": {},
   "source": [
    "**(2)** $$Burrow's  Delta(A_{j},Q) = \\displaystyle\\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\mid Zscore(t_{i,A{j}}) - Zscore(t_{i,Q}) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14b2d0",
   "metadata": {},
   "source": [
    "- Aj is a candidate author A's profile.\n",
    "- Q is the query text.\n",
    "- t is a set of word-types in the MFW list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c0911",
   "metadata": {},
   "source": [
    "Each *t* in the MFW list has the same importance, but the impact depends on their Z score values. <br>\n",
    "To get the Delta value between the query text and a sample text, a list of most frequent word-types is necessary. A relative frequency value for each term can be calculated for each text: the number of occurrences for a certain word-type in a certain text is divided by the length of the text.<br>\n",
    "Then the relative frequency values are compared against each other to get mean and standard deviation values. This is to get Z score for each term in each text: Z score is the relative frequency minus mean divided by standard deviation. Z score helps us understand where a certain value lies in relation to the entire sample. By comparing a Z score for a certain term in both texts, we know how much difference in using that word there is, and the bigger the sum is, the bigger the difference in word choices between the texts will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74134fd6",
   "metadata": {},
   "source": [
    "The function `MFW` below returns a list of 300 most frequent words and their frequency. The number 300 can be changed if necessary. Frequency here is absolute frequency, i.e. how many times it appears in the text.\n",
    "Function `MFW_100` returns the percentage of MFW tokens in relation to the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7371fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFW(text):\n",
    "    freq = FreqDist(text)\n",
    "    MFWlist = freq.most_common(300)\n",
    "    return MFWlist\n",
    "\n",
    "def MFW_100(text):\n",
    "    return 100 * sum(i[1] for i in MFW(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b110a",
   "metadata": {},
   "source": [
    "The codes below create a table of most frequent words (MFW) with their absolute frequency in the three respective texts. Obviously, the MFW list is different for each of the text with some overlap, and to be able to compare to each other, I took only MFWs that are present in all three lists, which makes the list shorter than the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b45c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_table(xa, xb, xc):\n",
    "    dict_b = (dict(MFW(xb)))\n",
    "    dict_c = (dict(MFW(xc)))\n",
    "    table = pd.DataFrame(MFW(xa)).rename(columns={0: 'word', 1:'a'})\n",
    "    table.set_index('word',inplace=True)\n",
    "    table[\"b\"] = \"\"\n",
    "    table[\"c\"] = \"\"\n",
    "    for n in MFW(xa):\n",
    "        word = n[0]\n",
    "        if dict_b.get(word) != None and dict_c.get(word) != None:\n",
    "            table.loc[word,\"b\"] = dict_b.get(word)\n",
    "            table.loc[word,\"c\"] = dict_c.get(word)\n",
    "        else:\n",
    "            table.loc[word,\"b\"] = np.nan\n",
    "            table.loc[word,\"c\"] = np.nan\n",
    "        table.dropna(inplace= True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3f0c3",
   "metadata": {},
   "source": [
    "The table we get from `abs_table` is then turned into a relative frequency table. Relative frequency table takes each text's length (number of tokens) into consideration. Since the absolute frequency of MFW will be heavily influenced by the size of the corpus, a relative term frequency is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_table(xa, xb, xc):\n",
    "    table = abs_table(xa, xb, xc)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(xa),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(xb),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(xc),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    return table\n",
    "\n",
    "table = rel_table(text_a,text_b,text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79698bb8",
   "metadata": {},
   "source": [
    "Lastly, the codes below calculate z-score and eventually Delta score. The query text has to be preprocessed before running these lines. <br>\n",
    "Since the function `zscore_table` takes the result from `abs_table` and calculates relative frequency in itself, the function `rel_table` above is not required if the goal is to calculate the Delta score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d3acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_table(a,b,c):\n",
    "    dict_q = dict(collections.Counter(q))\n",
    "    table = abs_table(a, b, c)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table[\"q\"] = \"\"\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(a),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(b),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(c),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    for word in table[\"words\"]:\n",
    "        if dict_q.get(word) != None:\n",
    "            table.loc[word,\"q\"] = round((dict_q.get(word) / len(q)),5)\n",
    "        else:\n",
    "            table.loc[word,\"q\"] = np.nan\n",
    "    table.loc[:,\"z_a\"] = (table[\"a\"] - table[\"mean\"]) / table[\"sd\"] # calculates z-scores for columns a,b,c,q\n",
    "    table.loc[:,\"z_b\"] = (table[\"b\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_c\"] = (table[\"c\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_q\"] = (table[\"q\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.dropna(inplace= True) # deletes rows that contain NaN\n",
    "    table.drop('words', axis = 'columns',inplace= True) # deletes the redundant column\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "549922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(df): # calculates delta score between the column a in the given dataframe and the query text\n",
    "    delta_a = round(sum(list(abs(df[\"z_a\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_b = round(sum(list(abs(df[\"z_b\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_c = round(sum(list(abs(df[\"z_c\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    return delta_a, delta_b, delta_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78c637",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9c217",
   "metadata": {},
   "source": [
    "In the previous section, several methods of characterizing writing styles for each author were introduced. In this section, I present results and examine the possibility of attributing correct authors to the query texts based on each index. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7b8d5",
   "metadata": {},
   "source": [
    "Query texts are from the same blogs the sample texts are from, but collected separately so that the query texts are not included in the sample. <br>\n",
    "First, importing the query texts the same way as the sample texts, using `preprocess` function defined earlier: the query text names indicate which author it was written by, for example `q_a` was written by `author_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d63d1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query text preprocessing\n",
    "\n",
    "q_a = preprocess('q1.txt')\n",
    "q_b = preprocess('q2.txt')\n",
    "q_c = preprocess('q3.txt')\n",
    "q_d = preprocess('q4.txt')\n",
    "q_e = preprocess('q5.txt')\n",
    "q_f = preprocess('q6.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9806e",
   "metadata": {},
   "source": [
    "### Results\n",
    "#### Lexical analysis\n",
    "##### Type-token ratio\n",
    "Type-token ratio of the sample texts were presented in Table2 (repeated here):\n",
    "![table2](Table2_typetoken_ratio.png)\n",
    "Type-token ratio of the query texts are as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305f2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2593\n",
      "0.3952\n",
      "0.3203\n",
      "0.3065\n",
      "0.3578\n",
      "0.3496\n"
     ]
    }
   ],
   "source": [
    "print(typetoken_ratio(q_a))\n",
    "print(typetoken_ratio(q_b))\n",
    "print(typetoken_ratio(q_c))\n",
    "print(typetoken_ratio(q_d))\n",
    "print(typetoken_ratio(q_e))\n",
    "print(typetoken_ratio(q_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22693f32",
   "metadata": {},
   "source": [
    "**Table 3**\n",
    "![table3](Table3_sample_query_ttratio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81377756",
   "metadata": {},
   "source": [
    "The type-token ratio of the query texts were much bigger than the sample texts, making it impossible as a useful tool for authorship attribution: for example, \n",
    "\n",
    "Type-token ratio was not a useful tool for authorship attribution, for two reasons:\n",
    "1. The type-token ratio difference between the sample texts as well as between the query texts were not big enough to be able to assign any query text to any sample text.\n",
    "2. The type-token ratio of the query texts were much bigger than the sample texts. (see Figure 1 below)\n",
    "\n",
    "Both are caused by the fact the query texts are much shorter than the sample texts. Type-token ratio is heavily influenced by the text length, because the number of word-types used in a text does not increase in the same rate as the text gets longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58452d",
   "metadata": {},
   "source": [
    "**Figure 1**\n",
    "![figure1](Figure1_ttratio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1e956",
   "metadata": {},
   "source": [
    "##### Simpson's D\n",
    "Simpson's D values from each sample text and query text are as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe714d0d",
   "metadata": {},
   "source": [
    "**Table 4**\n",
    "![table4](Table4_simpson_D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31f478",
   "metadata": {},
   "source": [
    "EXPLANATIONS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc907258",
   "metadata": {},
   "source": [
    "##### Big words index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc064f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1906\n",
      "0.2172\n",
      "0.2306\n",
      "0.1596\n",
      "0.2229\n",
      "0.1949\n",
      "0.1992\n",
      "0.1808\n",
      "0.1369\n",
      "0.131\n",
      "0.2044\n",
      "0.2089\n"
     ]
    }
   ],
   "source": [
    "print(BWI(text_a))\n",
    "print(BWI(text_b))\n",
    "print(BWI(text_c))\n",
    "print(BWI(text_d))\n",
    "print(BWI(text_e))\n",
    "print(BWI(text_f))\n",
    "\n",
    "print(BWI(q_a))\n",
    "print(BWI(q_b))\n",
    "print(BWI(q_c))\n",
    "print(BWI(q_d))\n",
    "print(BWI(q_e))\n",
    "print(BWI(q_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ee7ae",
   "metadata": {},
   "source": [
    "##### Mean sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5755c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_a = mean_sent('author_a.txt')\n",
    "sent_b = mean_sent('author_b.txt')\n",
    "sent_c = mean_sent('author_c.txt')\n",
    "sent_d = mean_sent('author_d.txt')\n",
    "sent_e = mean_sent('author_e.txt')\n",
    "sent_f = mean_sent('author_f.txt')\n",
    "\n",
    "q_sent_a = mean_sent('q1.txt')\n",
    "q_sent_b = mean_sent('q2.txt')\n",
    "q_sent_c = mean_sent('q3.txt')\n",
    "q_sent_d = mean_sent('q4.txt')\n",
    "q_sent_e = mean_sent('q5.txt')\n",
    "q_sent_f = mean_sent('q6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47be5fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.232032854209447\n",
      "18.158604282315622\n",
      "19.0187265917603\n",
      "16.016536661466457\n",
      "18.10102800425381\n",
      "19.19683257918552\n",
      "21.445026178010473\n",
      "17.857142857142858\n",
      "19.927272727272726\n",
      "15.862068965517242\n",
      "20.816326530612244\n",
      "19.14814814814815\n"
     ]
    }
   ],
   "source": [
    "print(sent_a)\n",
    "print(sent_b)\n",
    "print(sent_c)\n",
    "print(sent_d)\n",
    "print(sent_e)\n",
    "print(sent_f)\n",
    "\n",
    "print(q_sent_a)\n",
    "print(q_sent_b)\n",
    "print(q_sent_c)\n",
    "print(q_sent_d)\n",
    "print(q_sent_e)\n",
    "print(q_sent_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe9b9a",
   "metadata": {},
   "source": [
    "##### Mean word length<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbea7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.521466666666667\n",
      "4.681866666666667\n",
      "4.804866666666666\n",
      "4.290885005746791\n",
      "4.729686857411433\n",
      "4.532508348065213\n",
      "4.5224609375\n",
      "4.5728\n",
      "4.564796905222437\n",
      "4.203804347826087\n",
      "4.575980392156863\n",
      "4.564796905222437\n"
     ]
    }
   ],
   "source": [
    "print(mean_word(text_a_split))\n",
    "print(mean_word(text_b_split))\n",
    "print(mean_word(text_c_split))\n",
    "print(mean_word(text_d))\n",
    "print(mean_word(text_e))\n",
    "print(mean_word(text_f))\n",
    "\n",
    "print(mean_word(q_a))\n",
    "print(mean_word(q_b))\n",
    "print(mean_word(q_f))\n",
    "print(mean_word(q_d))\n",
    "print(mean_word(q_e))\n",
    "print(mean_word(q_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9e407",
   "metadata": {},
   "source": [
    "##### Word length distribution<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d5030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 788, 2: 3517, 3: 4083, 4: 4659, 5: 2399, 6: 2080, 7: 1615, 8: 915, 9: 776, 10: 434, 11: 199, 12: 105, 13: 50, 14: 21, 15: 8, 16: 3, 17: 1, 18: 1}\n",
      "{1: 918, 2: 3376, 3: 3645, 4: 4588, 5: 2804, 6: 2593, 7: 2031, 8: 1031, 9: 812, 10: 502, 11: 310, 12: 167, 13: 49, 14: 33, 15: 23, 16: 5, 17: 4, 18: 2, 19: 2, 20: 1, 21: 0, 22: 0, 23: 1}\n",
      "{1: 487, 2: 2037, 3: 2709, 4: 2682, 5: 2202, 6: 1604, 7: 1530, 8: 760, 9: 534, 10: 322, 11: 165, 12: 135, 13: 31, 14: 10, 15: 23, 16: 3}\n",
      "{1: 1457, 2: 8088, 3: 12764, 4: 11030, 5: 5571, 6: 4229, 7: 3473, 8: 2214, 9: 1237, 10: 649, 11: 340, 12: 126, 13: 105, 14: 43, 15: 3, 16: 2, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 1, 23: 0, 24: 0, 25: 0, 26: 1}\n",
      "{1: 1755, 2: 6742, 3: 10777, 4: 8647, 5: 6576, 6: 5185, 7: 4389, 8: 2887, 9: 1862, 10: 1037, 11: 600, 12: 240, 13: 278, 14: 49, 15: 20, 16: 8, 17: 2, 18: 3, 19: 3, 20: 1, 21: 0, 22: 2}\n",
      "{1: 1724, 2: 7923, 3: 10987, 4: 9566, 5: 6109, 6: 4678, 7: 3624, 8: 2689, 9: 1726, 10: 901, 11: 486, 12: 236, 13: 178, 14: 43, 15: 15, 16: 8, 17: 5, 18: 1, 19: 0, 20: 2, 21: 2, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 1, 30: 0, 31: 0, 32: 1, 33: 0, 34: 1, 35: 1, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 1, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 1, 55: 0, 56: 0, 57: 0, 58: 1}\n",
      "{1: 170, 2: 660, 3: 748, 4: 907, 5: 483, 6: 312, 7: 282, 8: 238, 9: 138, 10: 57, 11: 69, 12: 14, 13: 10, 14: 5, 15: 0, 16: 0, 17: 1, 18: 0, 19: 1, 20: 0, 21: 0, 22: 1}\n",
      "{1: 67, 2: 175, 3: 218, 4: 256, 5: 135, 6: 173, 7: 85, 8: 51, 9: 46, 10: 17, 11: 15, 12: 6, 13: 1, 14: 4, 15: 1}\n",
      "{1: 54, 2: 154, 3: 259, 4: 161, 5: 145, 6: 173, 7: 42, 8: 35, 9: 41, 10: 17, 11: 10, 12: 4, 13: 1}\n",
      "{1: 45, 2: 285, 3: 484, 4: 400, 5: 237, 6: 148, 7: 94, 8: 65, 9: 46, 10: 12, 11: 13, 12: 7, 13: 1, 14: 2, 15: 1}\n",
      "{1: 74, 2: 294, 3: 432, 4: 384, 5: 257, 6: 182, 7: 176, 8: 108, 9: 55, 10: 23, 11: 31, 12: 13, 13: 9, 14: 1, 15: 1}\n",
      "{1: 40, 2: 330, 3: 486, 4: 383, 5: 215, 6: 182, 7: 173, 8: 107, 9: 75, 10: 31, 11: 29, 12: 11, 13: 4, 14: 2}\n"
     ]
    }
   ],
   "source": [
    "print(wordlength(text_a))\n",
    "print(wordlength(text_b))\n",
    "print(wordlength(text_c))\n",
    "print(wordlength(text_d))\n",
    "print(wordlength(text_e))\n",
    "print(wordlength(text_f))\n",
    "\n",
    "print(wordlength(q_a))\n",
    "print(wordlength(q_b))\n",
    "print(wordlength(q_c))\n",
    "print(wordlength(q_d))\n",
    "print(wordlength(q_e))\n",
    "print(wordlength(q_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b295e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length(text,r):\n",
    "    word = []\n",
    "    for i in text:\n",
    "        if len(i) == r:\n",
    "            word.append(i)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af26c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manzanaresmedrano', 'neuropsychiatrist', 'turrubiatesimpson', 'complicatedasfuck', 'suspiciouslooking']\n",
      "['misrepresentations']\n",
      "['aaahhh…screaming…why', 'performanceenhancing']\n",
      "['jimandwafflesgmailcom', 'jimandwafflesgmailcom']\n",
      "['methylenedioxymethamphetamine']\n",
      "['httpswwwfacebookcomangelancraig1']\n",
      "['httpswwwlinkedincominzohrehsadeghi']\n",
      "['hwwwgofundmecomcontactsuggestdonor”']\n",
      "['httpsyoutubecomshortstaqrlasvl6gfeatureshare']\n",
      "['httpswwwfacebookcomprofilephpid586601275mibextidlqqj4d']\n",
      "['httpswwwinstagramcomstorieshighlights18051628441408424hlen']\n"
     ]
    }
   ],
   "source": [
    "print(find_length(text_f,18))\n",
    "print(find_length(text_f,20)) #11+9\n",
    "print(find_length(text_f,21))\n",
    "print(find_length(text_f,29))\n",
    "print(find_length(text_f,32))\n",
    "print(find_length(text_f,34))\n",
    "print(find_length(text_f,35))\n",
    "print(find_length(text_f,44))\n",
    "print(find_length(text_f,54))\n",
    "print(find_length(text_f,58))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77ae5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rehabilitating—or', 'justicefortiffany']\n",
      "['kidnappingsmissing', 'percivalcalderbank', 'vulnerable–newborn']\n",
      "['understandingmodern', 'internationalmodern', 'arresttrialsentence']\n",
      "['sweetheartturnedwife']\n",
      "['rehabilitate—convicted', 'thirtysomethingyearold']\n"
     ]
    }
   ],
   "source": [
    "print(find_length(text_e,17)) #2+14\n",
    "print(find_length(text_e,18)) #11+7, 10+7 c\n",
    "print(find_length(text_e,19)) #13+6, 13+6, 6+5+8 c\n",
    "print(find_length(text_e,20)) #10+6+4 c\n",
    "print(find_length(text_e,22)) #12+9 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de644e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4footby12footwide']\n",
      "['pollinatorfriendly']\n",
      "['morristownbenardsville']\n"
     ]
    }
   ],
   "source": [
    "print(find_length(text_a,17))\n",
    "print(find_length(text_a,18))\n",
    "print(find_length(text_d,22)) #10+12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54106ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['easily—particularly', 'lilyofthevalley—and']\n",
      "['insects—particularly']\n",
      "['maintenance—deadheading']\n"
     ]
    }
   ],
   "source": [
    "print(find_length(text_b,19)) #6+12, 4+2+3+6+3\n",
    "print(find_length(text_b,20)) #7+12\n",
    "print(find_length(text_b,23)) #11+11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e7a99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compartmentalized']\n",
      "['couplehundreddollar']\n",
      "['greenhousemegastorecom']\n"
     ]
    }
   ],
   "source": [
    "print(find_length(q_a,17))\n",
    "print(find_length(q_a,19))\n",
    "print(find_length(q_a,22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f036d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pollinatorfriendly\n",
      "maintenance—deadheading\n",
      "droughtresistant\n",
      "okieinvestigationsgmailcom\n",
      "rehabilitate—convicted\n",
      "httpswwwinstagramcomstorieshighlights18051628441408424hlen\n",
      "greenhousemegastorecom\n",
      "pressuretreated\n",
      "approximately\n",
      "christmasthemed\n",
      "coldbloodedness\n",
      "investigations\n"
     ]
    }
   ],
   "source": [
    "def longest_word(word_list):  \n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return longest_word\n",
    "\n",
    "print(longest_word(text_a))\n",
    "print(longest_word(text_b))\n",
    "print(longest_word(text_c))\n",
    "print(longest_word(text_d))\n",
    "print(longest_word(text_e))\n",
    "print(longest_word(text_f))\n",
    "print(longest_word(q_a))\n",
    "print(longest_word(q_b))\n",
    "print(longest_word(q_c))\n",
    "print(longest_word(q_d))\n",
    "print(longest_word(q_e))\n",
    "print(longest_word(q_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d999b0d",
   "metadata": {},
   "source": [
    "##### Lexical Density<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aa59c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5389\n",
      "0.5947\n",
      "0.6211\n",
      "0.4825\n",
      "0.5664\n",
      "0.5513\n",
      "0.5439\n",
      "0.5784\n",
      "0.6223\n",
      "0.4924\n",
      "0.5392\n",
      "0.5551\n"
     ]
    }
   ],
   "source": [
    "print(l_density(text_a))\n",
    "print(l_density(text_b))\n",
    "print(l_density(text_c))\n",
    "print(l_density(text_d))\n",
    "print(l_density(text_e))\n",
    "print(l_density(text_f))\n",
    "\n",
    "print(l_density(q_a))\n",
    "print(l_density(q_b))\n",
    "print(l_density(q_c))\n",
    "print(l_density(q_d))\n",
    "print(l_density(q_e))\n",
    "print(l_density(q_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6b7c3",
   "metadata": {},
   "source": [
    "### Distance-based analysis\n",
    "Burrow's Delta<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9002920e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04221</td>\n",
       "      <td>0.03389</td>\n",
       "      <td>0.06525</td>\n",
       "      <td>0.03931</td>\n",
       "      <td>0.047117</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>-0.369911</td>\n",
       "      <td>-0.997151</td>\n",
       "      <td>1.367061</td>\n",
       "      <td>-0.58854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.03131</td>\n",
       "      <td>0.02638</td>\n",
       "      <td>0.03020</td>\n",
       "      <td>0.02563</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>-1.381264</td>\n",
       "      <td>0.427797</td>\n",
       "      <td>-1.736446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03094</td>\n",
       "      <td>0.02913</td>\n",
       "      <td>0.03164</td>\n",
       "      <td>0.03491</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.349857</td>\n",
       "      <td>-1.361604</td>\n",
       "      <td>1.011748</td>\n",
       "      <td>4.103725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.02106</td>\n",
       "      <td>0.02371</td>\n",
       "      <td>0.02107</td>\n",
       "      <td>0.01709</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.711113</td>\n",
       "      <td>1.414206</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>-3.895082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.02064</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.02704</td>\n",
       "      <td>0.02563</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>-1.344843</td>\n",
       "      <td>1.051299</td>\n",
       "      <td>0.293544</td>\n",
       "      <td>-0.067413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-1.345530</td>\n",
       "      <td>1.049809</td>\n",
       "      <td>0.295721</td>\n",
       "      <td>8.147109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planted</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-1.336306</td>\n",
       "      <td>1.069045</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>-1.65702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>0.00210</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>-0.840256</td>\n",
       "      <td>-0.565000</td>\n",
       "      <td>1.405256</td>\n",
       "      <td>-0.927179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watering</th>\n",
       "      <td>0.00046</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-1.307403</td>\n",
       "      <td>0.186772</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>11.019539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>0.00046</td>\n",
       "      <td>0.00122</td>\n",
       "      <td>0.00217</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-1.176965</td>\n",
       "      <td>-0.090536</td>\n",
       "      <td>1.267500</td>\n",
       "      <td>-0.790997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a        b        c        q      mean        sd       z_a  \\\n",
       "word                                                                         \n",
       "the       0.04221  0.03389  0.06525  0.03931  0.047117  0.013264 -0.369911   \n",
       "and       0.03131  0.02638  0.03020  0.02563  0.029297  0.002112  0.953467   \n",
       "to        0.03094  0.02913  0.03164  0.03491  0.030570  0.001058  0.349857   \n",
       "of        0.02106  0.02371  0.02107  0.01709  0.021947  0.001247 -0.711113   \n",
       "a         0.02064  0.03000  0.02704  0.02563  0.025893  0.003906 -1.344843   \n",
       "...           ...      ...      ...      ...       ...       ...       ...   \n",
       "start     0.00055  0.00109  0.00092  0.00269  0.000853  0.000225 -1.345530   \n",
       "planted   0.00055  0.00100  0.00085  0.00049  0.000800  0.000187 -1.336306   \n",
       "green     0.00055  0.00074  0.00210  0.00049  0.001130  0.000690 -0.840256   \n",
       "watering  0.00046  0.00070  0.00085  0.00244  0.000670  0.000161 -1.307403   \n",
       "easy      0.00046  0.00122  0.00217  0.00073  0.001283  0.000700 -1.176965   \n",
       "\n",
       "               z_b       z_c        z_q  \n",
       "word                                     \n",
       "the      -0.997151  1.367061   -0.58854  \n",
       "and      -1.381264  0.427797  -1.736446  \n",
       "to       -1.361604  1.011748   4.103725  \n",
       "of        1.414206 -0.703093  -3.895082  \n",
       "a         1.051299  0.293544  -0.067413  \n",
       "...            ...       ...        ...  \n",
       "start     1.049809  0.295721   8.147109  \n",
       "planted   1.069045  0.267261   -1.65702  \n",
       "green    -0.565000  1.405256  -0.927179  \n",
       "watering  0.186772  1.120631  11.019539  \n",
       "easy     -0.090536  1.267500  -0.790997  \n",
       "\n",
       "[103 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_a\n",
    "abc_a = zscore_table(text_a, text_b, text_c)\n",
    "abc_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c16839ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04221</td>\n",
       "      <td>0.03389</td>\n",
       "      <td>0.06525</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.047117</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>-0.369911</td>\n",
       "      <td>-0.997151</td>\n",
       "      <td>1.367061</td>\n",
       "      <td>-1.923686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.03131</td>\n",
       "      <td>0.02638</td>\n",
       "      <td>0.03020</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>-1.381264</td>\n",
       "      <td>0.427797</td>\n",
       "      <td>-5.539264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03094</td>\n",
       "      <td>0.02913</td>\n",
       "      <td>0.03164</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.349857</td>\n",
       "      <td>-1.361604</td>\n",
       "      <td>1.011748</td>\n",
       "      <td>1.352149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.02106</td>\n",
       "      <td>0.02371</td>\n",
       "      <td>0.02107</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.711113</td>\n",
       "      <td>1.414206</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>-4.127664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.02064</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.02704</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>-1.344843</td>\n",
       "      <td>1.051299</td>\n",
       "      <td>0.293544</td>\n",
       "      <td>3.816078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>-1.135550</td>\n",
       "      <td>-0.162221</td>\n",
       "      <td>1.297771</td>\n",
       "      <td>-0.684935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-1.345530</td>\n",
       "      <td>1.049809</td>\n",
       "      <td>0.295721</td>\n",
       "      <td>3.312073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planted</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-1.336306</td>\n",
       "      <td>1.069045</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>0.00210</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>-0.840256</td>\n",
       "      <td>-0.565000</td>\n",
       "      <td>1.405256</td>\n",
       "      <td>-0.478077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>0.00046</td>\n",
       "      <td>0.00122</td>\n",
       "      <td>0.00217</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-1.176965</td>\n",
       "      <td>-0.090536</td>\n",
       "      <td>1.267500</td>\n",
       "      <td>-0.690931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               a        b        c       q      mean        sd       z_a  \\\n",
       "word                                                                       \n",
       "the      0.04221  0.03389  0.06525  0.0216  0.047117  0.013264 -0.369911   \n",
       "and      0.03131  0.02638  0.03020  0.0176  0.029297  0.002112  0.953467   \n",
       "to       0.03094  0.02913  0.03164   0.032  0.030570  0.001058  0.349857   \n",
       "of       0.02106  0.02371  0.02107  0.0168  0.021947  0.001247 -0.711113   \n",
       "a        0.02064  0.03000  0.02704  0.0408  0.025893  0.003906 -1.344843   \n",
       "...          ...      ...      ...     ...       ...       ...       ...   \n",
       "long     0.00055  0.00109  0.00190  0.0008  0.001180  0.000555 -1.135550   \n",
       "start    0.00055  0.00109  0.00092  0.0016  0.000853  0.000225 -1.345530   \n",
       "planted  0.00055  0.00100  0.00085  0.0008  0.000800  0.000187 -1.336306   \n",
       "green    0.00055  0.00074  0.00210  0.0008  0.001130  0.000690 -0.840256   \n",
       "easy     0.00046  0.00122  0.00217  0.0008  0.001283  0.000700 -1.176965   \n",
       "\n",
       "              z_b       z_c       z_q  \n",
       "word                                   \n",
       "the     -0.997151  1.367061 -1.923686  \n",
       "and     -1.381264  0.427797 -5.539264  \n",
       "to      -1.361604  1.011748  1.352149  \n",
       "of       1.414206 -0.703093 -4.127664  \n",
       "a        1.051299  0.293544  3.816078  \n",
       "...           ...       ...       ...  \n",
       "long    -0.162221  1.297771 -0.684935  \n",
       "start    1.049809  0.295721  3.312073  \n",
       "planted  1.069045  0.267261       0.0  \n",
       "green   -0.565000  1.405256 -0.478077  \n",
       "easy    -0.090536  1.267500 -0.690931  \n",
       "\n",
       "[84 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_b\n",
    "abc_b = zscore_table(text_a, text_b, text_c)\n",
    "abc_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "478854bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04221</td>\n",
       "      <td>0.03389</td>\n",
       "      <td>0.06525</td>\n",
       "      <td>0.10675</td>\n",
       "      <td>0.047117</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>-0.369911</td>\n",
       "      <td>-0.997151</td>\n",
       "      <td>1.367061</td>\n",
       "      <td>4.495722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.03131</td>\n",
       "      <td>0.02638</td>\n",
       "      <td>0.03020</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>-1.381264</td>\n",
       "      <td>0.427797</td>\n",
       "      <td>-3.502886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03094</td>\n",
       "      <td>0.02913</td>\n",
       "      <td>0.03164</td>\n",
       "      <td>0.03467</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.349857</td>\n",
       "      <td>-1.361604</td>\n",
       "      <td>1.011748</td>\n",
       "      <td>3.876791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.02106</td>\n",
       "      <td>0.02371</td>\n",
       "      <td>0.02107</td>\n",
       "      <td>0.03102</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.711113</td>\n",
       "      <td>1.414206</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>7.276878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.02064</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.02704</td>\n",
       "      <td>0.02555</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>-1.344843</td>\n",
       "      <td>1.051299</td>\n",
       "      <td>0.293544</td>\n",
       "      <td>-0.087893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-1.348907</td>\n",
       "      <td>0.306570</td>\n",
       "      <td>1.042337</td>\n",
       "      <td>8.49198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>-1.135550</td>\n",
       "      <td>-0.162221</td>\n",
       "      <td>1.297771</td>\n",
       "      <td>1.153575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>-1.407711</td>\n",
       "      <td>0.821165</td>\n",
       "      <td>0.586546</td>\n",
       "      <td>2.815423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.00177</td>\n",
       "      <td>0.00365</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>-1.211143</td>\n",
       "      <td>-0.026766</td>\n",
       "      <td>1.237908</td>\n",
       "      <td>5.011856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-1.117654</td>\n",
       "      <td>-0.191598</td>\n",
       "      <td>1.309252</td>\n",
       "      <td>-0.478995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            a        b        c        q      mean        sd       z_a  \\\n",
       "word                                                                     \n",
       "the   0.04221  0.03389  0.06525  0.10675  0.047117  0.013264 -0.369911   \n",
       "and   0.03131  0.02638  0.03020   0.0219  0.029297  0.002112  0.953467   \n",
       "to    0.03094  0.02913  0.03164  0.03467  0.030570  0.001058  0.349857   \n",
       "of    0.02106  0.02371  0.02107  0.03102  0.021947  0.001247 -0.711113   \n",
       "a     0.02064  0.03000  0.02704  0.02555  0.025893  0.003906 -1.344843   \n",
       "...       ...      ...      ...      ...       ...       ...       ...   \n",
       "each  0.00060  0.00096  0.00112  0.00274  0.000893  0.000217 -1.348907   \n",
       "long  0.00055  0.00109  0.00190  0.00182  0.001180  0.000555 -1.135550   \n",
       "size  0.00055  0.00074  0.00072  0.00091  0.000670  0.000085 -1.407711   \n",
       "cut   0.00055  0.00114  0.00177  0.00365  0.001153  0.000498 -1.211143   \n",
       "best  0.00051  0.00109  0.00203  0.00091  0.001210  0.000626 -1.117654   \n",
       "\n",
       "           z_b       z_c       z_q  \n",
       "word                                \n",
       "the  -0.997151  1.367061  4.495722  \n",
       "and  -1.381264  0.427797 -3.502886  \n",
       "to   -1.361604  1.011748  3.876791  \n",
       "of    1.414206 -0.703093  7.276878  \n",
       "a     1.051299  0.293544 -0.087893  \n",
       "...        ...       ...       ...  \n",
       "each  0.306570  1.042337   8.49198  \n",
       "long -0.162221  1.297771  1.153575  \n",
       "size  0.821165  0.586546  2.815423  \n",
       "cut  -0.026766  1.237908  5.011856  \n",
       "best -0.191598  1.309252 -0.478995  \n",
       "\n",
       "[71 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_c\n",
    "abc_c = zscore_table(text_a, text_b, text_c)\n",
    "abc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eb886d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.06208</td>\n",
       "      <td>0.05501</td>\n",
       "      <td>0.04867</td>\n",
       "      <td>0.05815</td>\n",
       "      <td>0.055253</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>1.246353</td>\n",
       "      <td>-0.044426</td>\n",
       "      <td>-1.201928</td>\n",
       "      <td>0.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03183</td>\n",
       "      <td>0.02540</td>\n",
       "      <td>0.02616</td>\n",
       "      <td>0.02989</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>1.405918</td>\n",
       "      <td>-0.835418</td>\n",
       "      <td>-0.570501</td>\n",
       "      <td>0.729683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.02975</td>\n",
       "      <td>0.02938</td>\n",
       "      <td>0.02927</td>\n",
       "      <td>0.03696</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.379972</td>\n",
       "      <td>-0.422109</td>\n",
       "      <td>-0.957863</td>\n",
       "      <td>36.4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.02566</td>\n",
       "      <td>0.01911</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>1.182293</td>\n",
       "      <td>-1.263186</td>\n",
       "      <td>0.080894</td>\n",
       "      <td>0.935878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.02197</td>\n",
       "      <td>0.01565</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>1.323248</td>\n",
       "      <td>-1.093783</td>\n",
       "      <td>-0.229465</td>\n",
       "      <td>1.028768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00163</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.081514</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>-1.263466</td>\n",
       "      <td>12.919955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>-1.382189</td>\n",
       "      <td>0.950255</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>-1.252609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>together</th>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1.837117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-1.111168</td>\n",
       "      <td>-0.202031</td>\n",
       "      <td>1.313198</td>\n",
       "      <td>-0.353553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.889001</td>\n",
       "      <td>-0.508001</td>\n",
       "      <td>1.397001</td>\n",
       "      <td>-0.22225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a        b        c        q      mean        sd       z_a  \\\n",
       "word                                                                         \n",
       "the       0.06208  0.05501  0.04867  0.05815  0.055253  0.005477  1.246353   \n",
       "to        0.03183  0.02540  0.02616  0.02989  0.027797  0.002869  1.405918   \n",
       "and       0.02975  0.02938  0.02927  0.03696  0.029467  0.000205  1.379972   \n",
       "was       0.02566  0.01911  0.02271    0.025  0.022493  0.002678  1.182293   \n",
       "that      0.02197  0.01565  0.01791   0.0212  0.018510  0.002615  1.323248   \n",
       "...           ...      ...      ...      ...       ...       ...       ...   \n",
       "off       0.00058  0.00067  0.00047  0.00163  0.000573  0.000082  0.081514   \n",
       "scene     0.00053  0.00071  0.00067  0.00054  0.000637  0.000077 -1.382189   \n",
       "together  0.00051  0.00053  0.00049  0.00054  0.000510  0.000016  0.000000   \n",
       "am        0.00049  0.00055  0.00065  0.00054  0.000563  0.000066 -1.111168   \n",
       "same      0.00047  0.00051  0.00071  0.00054  0.000563  0.000105 -0.889001   \n",
       "\n",
       "               z_b       z_c        z_q  \n",
       "word                                     \n",
       "the      -0.044426 -1.201928   0.528848  \n",
       "to       -0.835418 -0.570501   0.729683  \n",
       "and      -0.422109 -0.957863    36.4962  \n",
       "was      -1.263186  0.080894   0.935878  \n",
       "that     -1.093783 -0.229465   1.028768  \n",
       "...            ...       ...        ...  \n",
       "off       1.181952 -1.263466  12.919955  \n",
       "scene     0.950255  0.431934  -1.252609  \n",
       "together  1.224745 -1.224745   1.837117  \n",
       "am       -0.202031  1.313198  -0.353553  \n",
       "same     -0.508001  1.397001   -0.22225  \n",
       "\n",
       "[123 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_d\n",
    "def_d = zscore_table(text_d, text_e, text_f)\n",
    "def_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fe19fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.06208</td>\n",
       "      <td>0.05501</td>\n",
       "      <td>0.04867</td>\n",
       "      <td>0.06569</td>\n",
       "      <td>0.055253</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>1.246353</td>\n",
       "      <td>-0.044426</td>\n",
       "      <td>-1.201928</td>\n",
       "      <td>1.905436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03183</td>\n",
       "      <td>0.02540</td>\n",
       "      <td>0.02616</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>1.405918</td>\n",
       "      <td>-0.835418</td>\n",
       "      <td>-0.570501</td>\n",
       "      <td>0.220764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.02975</td>\n",
       "      <td>0.02938</td>\n",
       "      <td>0.02927</td>\n",
       "      <td>0.03186</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.379972</td>\n",
       "      <td>-0.422109</td>\n",
       "      <td>-0.957863</td>\n",
       "      <td>11.656704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.02566</td>\n",
       "      <td>0.01911</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>0.01225</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>1.182293</td>\n",
       "      <td>-1.263186</td>\n",
       "      <td>0.080894</td>\n",
       "      <td>-3.824406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.02197</td>\n",
       "      <td>0.01565</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>0.02108</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>1.323248</td>\n",
       "      <td>-1.093783</td>\n",
       "      <td>-0.229465</td>\n",
       "      <td>0.982875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-1.174630</td>\n",
       "      <td>-0.094728</td>\n",
       "      <td>1.269358</td>\n",
       "      <td>12.523073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>-1.382189</td>\n",
       "      <td>0.950255</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>29.846654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>together</th>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00196</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>88.794003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-1.111168</td>\n",
       "      <td>-0.202031</td>\n",
       "      <td>1.313198</td>\n",
       "      <td>36.011938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.889001</td>\n",
       "      <td>-0.508001</td>\n",
       "      <td>1.397001</td>\n",
       "      <td>8.636009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a        b        c        q      mean        sd       z_a  \\\n",
       "word                                                                         \n",
       "the       0.06208  0.05501  0.04867  0.06569  0.055253  0.005477  1.246353   \n",
       "to        0.03183  0.02540  0.02616  0.02843  0.027797  0.002869  1.405918   \n",
       "and       0.02975  0.02938  0.02927  0.03186  0.029467  0.000205  1.379972   \n",
       "was       0.02566  0.01911  0.02271  0.01225  0.022493  0.002678  1.182293   \n",
       "that      0.02197  0.01565  0.01791  0.02108  0.018510  0.002615  1.323248   \n",
       "...           ...      ...      ...      ...       ...       ...       ...   \n",
       "can       0.00053  0.00072  0.00096  0.00294  0.000737  0.000176 -1.174630   \n",
       "scene     0.00053  0.00071  0.00067  0.00294  0.000637  0.000077 -1.382189   \n",
       "together  0.00051  0.00053  0.00049  0.00196  0.000510  0.000016  0.000000   \n",
       "am        0.00049  0.00055  0.00065  0.00294  0.000563  0.000066 -1.111168   \n",
       "same      0.00047  0.00051  0.00071  0.00147  0.000563  0.000105 -0.889001   \n",
       "\n",
       "               z_b       z_c        z_q  \n",
       "word                                     \n",
       "the      -0.044426 -1.201928   1.905436  \n",
       "to       -0.835418 -0.570501   0.220764  \n",
       "and      -0.422109 -0.957863  11.656704  \n",
       "was      -1.263186  0.080894  -3.824406  \n",
       "that     -1.093783 -0.229465   0.982875  \n",
       "...            ...       ...        ...  \n",
       "can      -0.094728  1.269358  12.523073  \n",
       "scene     0.950255  0.431934  29.846654  \n",
       "together  1.224745 -1.224745  88.794003  \n",
       "am       -0.202031  1.313198  36.011938  \n",
       "same     -0.508001  1.397001   8.636009  \n",
       "\n",
       "[134 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_e\n",
    "def_e = zscore_table(text_d, text_e, text_f)\n",
    "def_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c81453f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/3586535669.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
      "/tmp/ipykernel_560/3586535669.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  table.loc[:,\"sd\"] = table.std(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>q</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>z_a</th>\n",
       "      <th>z_b</th>\n",
       "      <th>z_c</th>\n",
       "      <th>z_q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.06208</td>\n",
       "      <td>0.05501</td>\n",
       "      <td>0.04867</td>\n",
       "      <td>0.05416</td>\n",
       "      <td>0.055253</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>1.246353</td>\n",
       "      <td>-0.044426</td>\n",
       "      <td>-1.201928</td>\n",
       "      <td>-0.199611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.03183</td>\n",
       "      <td>0.02540</td>\n",
       "      <td>0.02616</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>1.405918</td>\n",
       "      <td>-0.835418</td>\n",
       "      <td>-0.570501</td>\n",
       "      <td>1.604606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.02975</td>\n",
       "      <td>0.02938</td>\n",
       "      <td>0.02927</td>\n",
       "      <td>0.03337</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>1.379972</td>\n",
       "      <td>-0.422109</td>\n",
       "      <td>-0.957863</td>\n",
       "      <td>19.011143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.02566</td>\n",
       "      <td>0.01911</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>0.02128</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>1.182293</td>\n",
       "      <td>-1.263186</td>\n",
       "      <td>0.080894</td>\n",
       "      <td>-0.453005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.02197</td>\n",
       "      <td>0.01565</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>1.323248</td>\n",
       "      <td>-1.093783</td>\n",
       "      <td>-0.229465</td>\n",
       "      <td>-0.237114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.081514</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>-1.263466</td>\n",
       "      <td>-1.141195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>0.00145</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-0.285598</td>\n",
       "      <td>-1.056712</td>\n",
       "      <td>1.342309</td>\n",
       "      <td>7.168503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.00145</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>-0.455826</td>\n",
       "      <td>-0.931470</td>\n",
       "      <td>1.387295</td>\n",
       "      <td>4.89517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-1.174630</td>\n",
       "      <td>-0.094728</td>\n",
       "      <td>1.269358</td>\n",
       "      <td>6.782542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>together</th>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>28.169132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a        b        c        q      mean        sd       z_a  \\\n",
       "word                                                                         \n",
       "the       0.06208  0.05501  0.04867  0.05416  0.055253  0.005477  1.246353   \n",
       "to        0.03183  0.02540  0.02616   0.0324  0.027797  0.002869  1.405918   \n",
       "and       0.02975  0.02938  0.02927  0.03337  0.029467  0.000205  1.379972   \n",
       "was       0.02566  0.01911  0.02271  0.02128  0.022493  0.002678  1.182293   \n",
       "that      0.02197  0.01565  0.01791  0.01789  0.018510  0.002615  1.323248   \n",
       "...           ...      ...      ...      ...       ...       ...       ...   \n",
       "off       0.00058  0.00067  0.00047  0.00048  0.000573  0.000082  0.081514   \n",
       "last      0.00058  0.00049  0.00077  0.00145  0.000613  0.000117 -0.285598   \n",
       "area      0.00055  0.00047  0.00086  0.00145  0.000627  0.000168 -0.455826   \n",
       "can       0.00053  0.00072  0.00096  0.00193  0.000737  0.000176 -1.174630   \n",
       "together  0.00051  0.00053  0.00049  0.00097  0.000510  0.000016  0.000000   \n",
       "\n",
       "               z_b       z_c        z_q  \n",
       "word                                     \n",
       "the      -0.044426 -1.201928  -0.199611  \n",
       "to       -0.835418 -0.570501   1.604606  \n",
       "and      -0.422109 -0.957863  19.011143  \n",
       "was      -1.263186  0.080894  -0.453005  \n",
       "that     -1.093783 -0.229465  -0.237114  \n",
       "...            ...       ...        ...  \n",
       "off       1.181952 -1.263466  -1.141195  \n",
       "last     -1.056712  1.342309   7.168503  \n",
       "area     -0.931470  1.387295    4.89517  \n",
       "can      -0.094728  1.269358   6.782542  \n",
       "together  1.224745 -1.224745  28.169132  \n",
       "\n",
       "[117 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q_f\n",
    "def_f = zscore_table(text_d, text_e, text_f)\n",
    "def_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a1afbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.49817, 2.73929, 3.01049, 'are Delta distance values between the query text and text a, b, c, respectively.')\n",
      "(3.11836, 2.72426, 3.34396, 'are Delta distance values between the query text and text a, b, c, respectively.')\n",
      "(4.97545, 4.91509, 4.62071, 'are Delta distance values between the query text and text a, b, c, respectively.')\n",
      "(3.65058, 4.21731, 4.50348, 'are Delta distance values between the query text and text a, b, c, respectively.')\n",
      "(4.93397, 4.54952, 5.04095, 'are Delta distance values between the query text and text a, b, c, respectively.')\n",
      "(3.9648, 3.74926, 3.4331, 'are Delta distance values between the query text and text a, b, c, respectively.')\n"
     ]
    }
   ],
   "source": [
    "print(delta(abc_a))\n",
    "print(delta(abc_b))\n",
    "print(delta(abc_c))\n",
    "print(delta(def_d))\n",
    "print(delta(def_e))\n",
    "print(delta(def_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d24fd",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e36649",
   "metadata": {},
   "source": [
    "### Limitations and ways forward\n",
    "Interpret the results\n",
    "Ways forward (What more could be done, what more am I interested in, what I will do next)\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b735c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60fb955",
   "metadata": {},
   "source": [
    "Limitations\n",
    "1. an unrelated query text (that is NOT to be attributed to any of the authors, in other words) might have been useful to see how effective these methods are.\n",
    "2. some codes are written to be applied for cases with exactly three candidate authors, which makes it inapplicable for many other types of tasks\n",
    "\n",
    "\n",
    "Idea for future research:\n",
    "- what do humans see when they are asked to attribute text to author, or asked \"does this text look like (insert name here) wrote it?\"\n",
    "- can we rewrite any text to mimic somebody specific's writing?\n",
    "- can we make the model 'understand' the 'core idea' behind the text as a tool for authorship attribution: like, 'this looks like someone wrote it, not because of the textual style itself, but because it is something that person surely would have written'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd9797",
   "metadata": {},
   "source": [
    "### Bibliography\n",
    "\n",
    "Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with python. O’Reilly Media. <br>\n",
    "Burrows, J. (2002). ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. Lit Linguist Computing, 17(3), 267-287. https://doi.org/10.1093/llc/17.3.267 <br>\n",
    "Coulthard, M., Johnson, A., & Wright, D. (2017). An introduction to forensic linguistics : language in evidence (Second edition. ed.). Routledge. <br>\n",
    "Crystal, D. (2008). 'Think on my words' : exploring Shakespeare's language. Cambridge University Press. <br>\n",
    "Hardcastle, R. A. (1993). Forensic linguistics: an assessment of the CUSUM method for the determination of authorship. Journal - Forensic Science Society, 33(2), 95-106. https://doi.org/10.1016/S0015-7368(93)72987-4 <br>\n",
    "Hardcastle, R. A. (1997). CUSUM: a credible method for the determination of authorship? Sci Justice, 37(2), 129-138. https://doi.org/10.1016/S1355-0306(97)72158-0 <br>\n",
    "Koppel, M., Schler, J., & Argamon, S. (2013). Authorship attribution: what's easy and what's hard? Journal of law and policy, 21(2), 317. <br>\n",
    "Koppel, M., Schler, J., Argamon, S., & Winter, Y. (2012). The \"Fundamental Problem\" of Authorship Attribution. English studies, 93(3), 284-291. https://doi.org/10.1080/0013838X.2012.668794 <br>\n",
    "Koppel, M., & Winter, Y. (2014). Determining if two documents are written by the same author. J Assn Inf Sci Tec, 65(1), 178-187. https://doi.org/10.1002/asi.22954 <br>\n",
    "Labbé, D. (2007). Experiments on authorship attribution by intertextual distance in english. Journal of quantitative linguistics, 14(1), 33-80. https://doi.org/10.1080/09296170600850601 <br>\n",
    "Morton, A. Q. (1991). Proper words on proper places. Department of Computing Science Research Report, R18, University of Glasgow.<br> \n",
    "Morton, A. Q., Michaelson, S. . (1990). The Q-Sum Plot. internal report CSR-3-90, Department of Computer Science, University of Edinburgh. <br>\n",
    "Savoy, J. (2020). Machine learning methods for stylometry : authorship attribution and author profiling (1st 2020. ed.). Springer. <br>\n",
    "Simpson, E. H. (1949). Measurement of Diversity. Nature (London), 163(4148), 688-688. https://doi.org/10.1038/163688a0 <br>\n",
    "Totty, R. N., Hardcastle, R. A., & Pearson, J. (1987). Forensic linguistics: the determination of authorship from habits of style. Journal - Forensic Science Society, 27(1), 13-28. https://doi.org/10.1016/S0015-7368(87)72702-9 <br>\n",
    "Zipf, G. K. (1932). Selected studies of the principle of relative frequency in language  [doi:10.4159/harvard.9780674434929]. Harvard Univ. Press. https://doi.org/10.4159/harvard.9780674434929 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eddb7f",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dea83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
