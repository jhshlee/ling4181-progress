{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d54f94",
   "metadata": {},
   "source": [
    "# Appendix 1. Codes (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc01db",
   "metadata": {},
   "source": [
    "The following is the entirety of the codes included in the main text.\n",
    "\n",
    "Data used in the paper can be found at https://github.com/jhshlee/ling4181-progress/tree/main\n",
    "\n",
    "The following codes are written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for everything except mean sentence length\n",
    "\n",
    "def preprocess(filename):\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    return text.translate(str.maketrans(\"\",\"\", string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing:\n",
    "text_a = preprocess('author_a.txt') # sample texts\n",
    "text_b = preprocess('author_b.txt')\n",
    "text_c = preprocess('author_c.txt')\n",
    "text_d = preprocess('author_d.txt')\n",
    "text_e = preprocess('author_e.txt')\n",
    "text_f = preprocess('author_f.txt')\n",
    "\n",
    "q_a = preprocess('q1.txt') # query texts\n",
    "q_b = preprocess('q2.txt')\n",
    "q_c = preprocess('q3.txt')\n",
    "q_d = preprocess('q4.txt')\n",
    "q_e = preprocess('q5.txt')\n",
    "q_f = preprocess('q6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Analysis\n",
    "## type-token ratio\n",
    "def typetoken_ratio(text):\n",
    "    return round(len(set(text))/len(text),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1727c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simpson's D\n",
    "def simpson_D(text):\n",
    "    count = collections.Counter(text)\n",
    "    types = set(text)\n",
    "    n = len(text)\n",
    "    def VOC(r):\n",
    "        VOC = 0\n",
    "        for i in types: # i is a word(type)\n",
    "            if count.get(i) == r:\n",
    "                VOC += 1\n",
    "        return VOC\n",
    "    if sum(VOC(r) for r in range(1, n)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(sum(VOC(r) * (r**2 - r) / (n**2 - n) for r in range(1,n+1)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e528df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Big Word index\n",
    "def BWI(text):\n",
    "    big_word = 0\n",
    "    for word in text:\n",
    "        if len(word) >= 7:\n",
    "            big_word += 1\n",
    "    return big_word / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd286dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean sentence length\n",
    "def mean_sent(filename): # put raw text, not split by space or deleted punctuation marks!\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    t = sent_tokenize(text)\n",
    "    split = []\n",
    "    for sent in t:\n",
    "        a = sent.translate(str.maketrans(\"\",\"\", string.punctuation)).split()\n",
    "        split.append(a)\n",
    "    return sum(len(sent) for sent in split) / len(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d030de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean word length\n",
    "def mean_word(text):\n",
    "    return sum(len(word) for word in text) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7728e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting sample texts to match lengths\n",
    "text_a_5k = text_a[:15000]\n",
    "text_b_5k = text_b[:15000]\n",
    "text_c_5k = text_c[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word length distribution\n",
    "def wordlength(text):\n",
    "    dist = {}\n",
    "    n = len(max(text, key=len))\n",
    "    X = list(i for i in range(1,n+1))\n",
    "    def length(r):\n",
    "        length = 0\n",
    "        for word in text:\n",
    "            if len(word) == r:\n",
    "                length += 1\n",
    "        return length\n",
    "    for x in X:\n",
    "        dist[x] = length(x)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lexical density\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def l_density(text):\n",
    "    filtered = []\n",
    "    for w in text:\n",
    "        if w not in stopwords:\n",
    "            filtered.append(w)\n",
    "    return round(len(filtered) / len(text),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2addc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-based analysis (Burrows' Delta)\n",
    "## Most frequent word list\n",
    "def MFW(text):\n",
    "    freq = FreqDist(text)\n",
    "    MFWlist = freq.most_common(300)\n",
    "    return MFWlist\n",
    "\n",
    "def MFW_100(text):\n",
    "    return 100 * sum(i[1] for i in MFW(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Absolute frequency table\n",
    "def abs_table(xa, xb, xc):\n",
    "    dict_b = (dict(MFW(xb)))\n",
    "    dict_c = (dict(MFW(xc)))\n",
    "    table = pd.DataFrame(MFW(xa)).rename(columns={0: 'word', 1:'a'})\n",
    "    table.set_index('word',inplace=True)\n",
    "    table[\"b\"] = \"\"\n",
    "    table[\"c\"] = \"\"\n",
    "    for n in MFW(xa):\n",
    "        word = n[0]\n",
    "        if dict_b.get(word) != None and dict_c.get(word) != None:\n",
    "            table.loc[word,\"b\"] = dict_b.get(word)\n",
    "            table.loc[word,\"c\"] = dict_c.get(word)\n",
    "        else:\n",
    "            table.loc[word,\"b\"] = np.nan\n",
    "            table.loc[word,\"c\"] = np.nan\n",
    "        table.dropna(inplace= True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relative frequency table\n",
    "def rel_table(xa, xb, xc):\n",
    "    table = abs_table(xa, xb, xc)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(xa),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(xb),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(xc),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-score table\n",
    "def zscore_table(a,b,c):\n",
    "    dict_q = dict(collections.Counter(q))\n",
    "    table = abs_table(a, b, c)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table[\"q\"] = \"\"\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(a),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(b),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(c),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    for word in table[\"words\"]:\n",
    "        if dict_q.get(word) != None:\n",
    "            table.loc[word,\"q\"] = round((dict_q.get(word) / len(q)),5)\n",
    "        else:\n",
    "            table.loc[word,\"q\"] = np.nan\n",
    "    table.loc[:,\"z_a\"] = (table[\"a\"] - table[\"mean\"]) / table[\"sd\"] # calculates z-scores for columns a,b,c,q\n",
    "    table.loc[:,\"z_b\"] = (table[\"b\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_c\"] = (table[\"c\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_q\"] = (table[\"q\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.dropna(inplace= True) # deletes rows that contain NaN\n",
    "    table.drop('words', axis = 'columns',inplace= True) # deletes the redundant column\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delta score\n",
    "def delta(df): # calculates delta score between the column a in the given dataframe and the query text\n",
    "    delta_a = round(sum(list(abs(df[\"z_a\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_b = round(sum(list(abs(df[\"z_b\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_c = round(sum(list(abs(df[\"z_c\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    return delta_a, delta_b, delta_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d49fd5",
   "metadata": {},
   "source": [
    "## Tables and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables 1-3 (type-token ratio):\n",
    "\n",
    "# Number of tokens\n",
    "print(len(text_a))\n",
    "print(len(text_b))\n",
    "print(len(text_c))\n",
    "print(len(text_d))\n",
    "print(len(text_e))\n",
    "print(len(text_f))\n",
    "\n",
    "# Number of types\n",
    "print(len(set(text_a)))\n",
    "print(len(set(text_b)))\n",
    "print(len(set(text_c)))\n",
    "print(len(set(text_d)))\n",
    "print(len(set(text_e)))\n",
    "print(len(set(text_f)))\n",
    "\n",
    "tt = pd.DataFrame({'token':[21654, 22897, 15234, 51333, 51063, 50910],\n",
    "                        'type':[3560, 3640, 2287, 4848, 6679, 7382]}, index = ['a', 'b', 'c', 'd','e','f'])\n",
    "ttratio = tt.loc[:,\"ratio\"] = round(ttratio[\"type\"] / ttratio[\"token\"],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tt) # Table 1\n",
    "print(ttratio) # Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d98c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(typetoken_ratio(q_a))\n",
    "print(typetoken_ratio(q_b))\n",
    "print(typetoken_ratio(q_c))\n",
    "print(typetoken_ratio(q_d))\n",
    "print(typetoken_ratio(q_e))\n",
    "print(typetoken_ratio(q_f))\n",
    "\n",
    "# Table 3\n",
    "ttratio_q = pd.DataFrame({'sample':[0.1644,0.1590,0.1501,0.0944,0.1308,0.1450],\n",
    "                        'query':[0.2593,0.3952,0.3203,0.3065,0.3578,0.3496]}, index = ['a', 'b', 'c', 'd','e','f'])\n",
    "print(ttratio_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 5 (Simpson's D)\n",
    "print(simpson_D(text_a))\n",
    "print(simpson_D(text_b))\n",
    "print(simpson_D(text_c))\n",
    "print(simpson_D(text_d))\n",
    "print(simpson_D(text_e))\n",
    "print(simpson_D(text_f))\n",
    "print(simpson_D(q_a))\n",
    "print(simpson_D(q_b))\n",
    "print(simpson_D(q_c))\n",
    "print(simpson_D(q_d))\n",
    "print(simpson_D(q_e))\n",
    "print(simpson_D(q_f)) # these results were compiled to a .csv file and imported  back to create a dataframe:\n",
    "\n",
    "simpsonD = pd.read_csv('Simpson_D.csv')\n",
    "simpsonD.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73750bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 7 (Big Words Index)\n",
    "print(BWI(text_a))\n",
    "print(BWI(text_b))\n",
    "print(BWI(text_c))\n",
    "print(BWI(text_d))\n",
    "print(BWI(text_e))\n",
    "print(BWI(text_f))\n",
    "print(BWI(q_a))\n",
    "print(BWI(q_b))\n",
    "print(BWI(q_c))\n",
    "print(BWI(q_d))\n",
    "print(BWI(q_e))\n",
    "print(BWI(q_f)) # these results were compiled to a .csv file and imported  back to create a dataframe:\n",
    "\n",
    "BWI = pd.read_csv('BWI.csv')\n",
    "BWI.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6948eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 9 (Mean sentence length)\n",
    "\n",
    "sent_a = mean_sent('author_a.txt')\n",
    "sent_b = mean_sent('author_b.txt')\n",
    "sent_c = mean_sent('author_c.txt')\n",
    "sent_d = mean_sent('author_d.txt')\n",
    "sent_e = mean_sent('author_e.txt')\n",
    "sent_f = mean_sent('author_f.txt')\n",
    "q_sent_a = mean_sent('q1.txt')\n",
    "q_sent_b = mean_sent('q2.txt')\n",
    "q_sent_c = mean_sent('q3.txt')\n",
    "q_sent_d = mean_sent('q4.txt')\n",
    "q_sent_e = mean_sent('q5.txt')\n",
    "q_sent_f = mean_sent('q6.txt')\n",
    "\n",
    "print(sent_a)\n",
    "print(sent_b)\n",
    "print(sent_c)\n",
    "print(sent_d)\n",
    "print(sent_e)\n",
    "print(sent_f)\n",
    "print(q_sent_a)\n",
    "print(q_sent_b)\n",
    "print(q_sent_c)\n",
    "print(q_sent_d)\n",
    "print(q_sent_e)\n",
    "print(q_sent_f) # these results were compiled to a .csv file and imported  back to create a dataframe:\n",
    "\n",
    "MSL = pd.read_csv('MSL.csv')\n",
    "MSL.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 11 (Mean word length)\n",
    "print(mean_word(text_a_split))\n",
    "print(mean_word(text_b_split))\n",
    "print(mean_word(text_c_split))\n",
    "print(mean_word(text_d))\n",
    "print(mean_word(text_e))\n",
    "print(mean_word(text_f))\n",
    "print(mean_word(q_a))\n",
    "print(mean_word(q_b))\n",
    "print(mean_word(q_f))\n",
    "print(mean_word(q_d))\n",
    "print(mean_word(q_e))\n",
    "print(mean_word(q_f)) # these results were compiled to a .csv file and imported  back to create a dataframe:\n",
    "\n",
    "MWL = pd.read_csv('MWL.csv')\n",
    "MWL.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c13d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for data correction for word length distribution graph (figure 2&3)\n",
    "def find_length(text,r):\n",
    "    word = []\n",
    "    for i in text:\n",
    "        if len(i) == r:\n",
    "            word.append(i)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 13 (Lexical Density)\n",
    "print(l_density(text_a))\n",
    "print(l_density(text_b))\n",
    "print(l_density(text_c))\n",
    "print(l_density(text_d))\n",
    "print(l_density(text_e))\n",
    "print(l_density(text_f))\n",
    "\n",
    "print(l_density(q_a))\n",
    "print(l_density(q_b))\n",
    "print(l_density(q_c))\n",
    "print(l_density(q_d))\n",
    "print(l_density(q_e))\n",
    "print(l_density(q_f)) # these results were compiled to a .csv file and imported  back to create a dataframe:\n",
    "\n",
    "LD = pd.read_csv('LD.csv')\n",
    "LD.set_index('index',inplace=True)\n",
    "LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burrows' Delta\n",
    "q = q_a\n",
    "abc_a = zscore_table(text_a, text_b, text_c) # Table 15\n",
    "q = q_b\n",
    "abc_b = zscore_table(text_a, text_b, text_c)\n",
    "q = q_c\n",
    "abc_c = zscore_table(text_a, text_b, text_c)\n",
    "q = q_d\n",
    "def_d = zscore_table(text_d, text_e, text_f)\n",
    "q = q_e\n",
    "def_e = zscore_table(text_d, text_e, text_f)\n",
    "q = q_f\n",
    "def_f = zscore_table(text_d, text_e, text_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17799e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delta(abc_a))\n",
    "print(delta(abc_b))\n",
    "print(delta(abc_c))\n",
    "print(delta(def_d))\n",
    "print(delta(def_e))\n",
    "print(delta(def_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 17 (F1-score)\n",
    "f1 = pd.read_csv('f1_score.csv')\n",
    "f1.set_index('index',inplace=True)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60144e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 18\n",
    "assigned = pd.read_csv('assigned_table.csv')\n",
    "assigned.set_index('data',inplace=True)\n",
    "assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb56e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 19\n",
    "acc = pd.read_csv('ans_rate.csv')\n",
    "acc.set_index('data',inplace=True)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
