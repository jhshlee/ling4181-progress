{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36afeb4f",
   "metadata": {},
   "source": [
    "# LING4181 Supervised Reading\n",
    "# Spring 2023\n",
    "# Jihyeong Lee\n",
    "# 1 June 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fefc4",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "* What is authorship attribution?\n",
    "* What approaches are there?\n",
    "* What is it useful for?\n",
    "* What was the purpose of me doing this & why reduplication?\n",
    "\n",
    "## 2. Background\n",
    "* Literature review; summarize important concepts, approaches, methods, Recent developments surrounding machine learning models/large language models, Limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb0ac6",
   "metadata": {},
   "source": [
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ea901",
   "metadata": {},
   "source": [
    "In this section, I explain in detail the data collection procedure and methods of analysis.\n",
    "As was mentioned in Section 2(Literature), there are various environments to authorship attribution: dichotomous attribution(one candidate), closed-pool attribution(determined number of candidates; say, 3 candidates, one of which the text can be attributed to), infinite-pool attribution(the author can be anyone), etc. In real-world authorship attribution problems, there are also instances of co-authorship, which requires more complex methods of analysis.\n",
    "The methods employed in this essay are applicable for closed-pool, single-author problems, and the data were collected accordingly. The methods can be divided to two large sections: lexical analysis and distance-based analysis. I collected data and wrote the codes myself, but the formula for each quantitative analysis largely followed chapters 2 and 3 from Savoy(2020).\n",
    "\n",
    "### Data collection\n",
    "Two sets of data consisted with texts from three authors each were collected. I call them `data1` and `data2` respectively.\n",
    "As the writing style of an individual varies across themes (what the text is about) and channels (what platform the text was intended for), I decided one channel and two themes: about gardening and true crime, on blogs run by private individuals. Sample blogs are found after simple google searches for each keyword \"gardening\" and \"true crime\" and three random blogs each with enough amount of text were selected. In total, texts from 3 gardening-related blogs were included in the first dataset (`data1`), and three true-crime blogs were included in the other (`data2`). Texts were manually collected by copy-pasting. A short excerpt from each author were stored separately as test(query) text. The query text is not included in the sample text. (In this essay, \"text\" refer to each sample text whose author is the same, unless otherwise noted.)\n",
    "\n",
    "Things worth noting about texts and collecting them:\n",
    "1. I do not personally know any of the blog owners that were included in the sample. (Links to the source blogs are included in the bibliography.)\n",
    "2. Author C in data1 was one of three co-writers in the blog, so I filtered out the two other authors and only included one.\n",
    "3. Some manual processing was involved to delete links, advertisement, mentions of the names of the blog owners.\n",
    "\n",
    "### Blog posts as a text\n",
    "I use blog posts as sample texts for the experiment. Blogs, like other channels, show several characteristic features which should be considered when analyzed:\n",
    "1. Blogs are online platforms for individuals or groups of people to share thoughts, experiences, or knowledge on various topics. Blog posts are typically reverse-chronologically organized, though there is no set rule for organizing posts.\n",
    "2. There is no set rules for the type of content, formality or layout for blog posts. Therefore, blog writers have a flexibility of writing styles. Some blogs deal with a single topic seriously, while others have different purposes.\n",
    "3. Many different types of media including images, audio files, and videos can be incorporated into the posts. These can be embedded inside the posts so that the readers can directly check them out without leaving the page, or provided as hyperlinks to external websites. This implies that a lot can be explained without using written words, and there can be mixed-medium contexts where the context cannot be understood entirely if only the text is considered.\n",
    "\n",
    "I presumed that inter-genre variability within a single individual would be bigger than interpersonal variability within a single genre, but blogs might be a different story. Blogs operate under self-determined rules, which might lead us to predict that there can be more room for variability between blogs than, say, tweets.\n",
    "\n",
    "Sample texts as well as full codes can be seen on ((LINK)). Codes are in addition attached as appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218d4a7",
   "metadata": {},
   "source": [
    "**Table 1**<br>\n",
    "Summary of sample data (texts a, b, c belong to `data1`, while d, e, f belong to `data2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dd721",
   "metadata": {},
   "source": [
    "![table1](Table1_type_token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e5af9",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "First, necessary packages are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8006577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e19029",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Each text went through preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d56e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    return text.translate(str.maketrans(\"\",\"\", string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eca73a",
   "metadata": {},
   "source": [
    "As such, all letters were converted to lowercase letters, and was removed punctuation marks (more about this later), then split to word units instead of letter unit strings. In other words, all texts after this are a list of all the words it contains, not a string of letters, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fefac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these', 'weeks', 'just', 'after', 'the', 'calendar', 'turns', 'from', 'one', 'year', 'to', 'the', 'next', 'are', 'the', 'perfect', 'time', 'to', 'think', 'about', 'your', 'goals', 'for', 'the', 'coming', 'gardening', 'season', 'on', 'this', 'weekâ€™s', 'podcast', 'i', 'discuss', 'plotting', 'out', 'plans', 'for', 'doubling', 'down', 'on', 'what', 'worked', 'well', 'in', 'the', 'garden', 'while', 'also', 'deciding', 'on']\n"
     ]
    }
   ],
   "source": [
    "text_a = preprocess('author_a.txt')\n",
    "text_b = preprocess('author_b.txt')\n",
    "text_c = preprocess('author_c.txt')\n",
    "text_d = preprocess('author_d.txt')\n",
    "text_e = preprocess('author_e.txt')\n",
    "text_f = preprocess('author_f.txt')\n",
    "print(text_a[0:50]) # processed text looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788064a0",
   "metadata": {},
   "source": [
    "### Lexical Analysis\n",
    "Lexical analysis methods refer to methods that make use of surface lexical information. There have been suggested several such methods that utilize different aspects of the usage of words.\n",
    "\n",
    "#### Lexical Diversity\n",
    "One way of quantitatively evaluating word choice is to measure the diversity of word use - in other words, how many types of words were used in relation to the total length (number of word-tokens) in a text.\n",
    "\n",
    "##### Type-token ratio\n",
    "Type-token ratio is a simple index for measuring lexical diversity in a given text. A low type-token ratio indicates low degree of diversity in the choice of words the author made. It can be simply calculated as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c23b5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def typetoken_ratio(text):\n",
    "    return round(len(set(text))/len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa3bb7",
   "metadata": {},
   "source": [
    "Table 1 is completed as the following: <br>\n",
    "**Table 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ce59",
   "metadata": {},
   "source": [
    "![table2](Table2_typetoken_ratio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe2ad",
   "metadata": {},
   "source": [
    "##### Simpson's D\n",
    "Among other ways to measure lexical diversity that were introduced in Savoy(2020), Simpson's D(1949) is a useful index that is less influenced by the text length. Simpson's D measures vocabulary richness by calculating the sum of probabilities \n",
    "of selecting the same word twice in two separate trials. The formula is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b60ac",
   "metadata": {},
   "source": [
    "**(1)** $$Simpson's  D(T) = \\displaystyle\\sum_{r=1} \\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot \\mid Voc_{r}(T) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583330e4",
   "metadata": {},
   "source": [
    "- T refers to the text.\n",
    "- n refers to the corpus size, i.e. the number of tokens in T.\n",
    "- r refers to the number of times a given word type appears in T.\n",
    "- VOC<sub>r</sub>(T) refers to the number of word types that appear in T exactly r times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fd62",
   "metadata": {},
   "source": [
    "When there is no diversity in the usage of words, in other words there are only one word that is used throughout the entire text ($ r=n $), the formula returns 1, which is the maximum value, Hence, the closer to 0 Simpson's D value is, the richer the vocabulary use is for the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7c7ba",
   "metadata": {},
   "source": [
    "The formula in **(1)** is rewritten as codes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson_D(text):\n",
    "    count = collections.Counter(text)\n",
    "    types = set(text)\n",
    "    n = len(text)\n",
    "    def VOC(r):\n",
    "        VOC = 0\n",
    "        for i in types: # i is a word(type)\n",
    "            if count.get(i) == r:\n",
    "                VOC += 1\n",
    "        return VOC\n",
    "    if sum(VOC(r) for r in range(1, n)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(sum(VOC(r) * (r**2 - r) / (n**2 - n) for r in range(1,n+1)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261e2d",
   "metadata": {},
   "source": [
    "The function `simpson_D(text)` takes a preprocessed text as its argument. `collections.Counter` function creates a dictionary type data where the key is the word type and the value is its occurence in the text. `VOC(r)` is an inner function that is defined as the number of word types(`i`) in `text` that appears r times. Since it presupposes `i` appears at least once in `text`, `VOC(r)` always appears as a positive number. Hence the absolute value sign in **(1)** is unnecessary. The function, then, calculates the sum of $\\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot Voc_{r}$ for all $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f771c",
   "metadata": {},
   "source": [
    "By  definition, the function should return 1 when $r=n$. However, the code above somehow always returns `0.0`. From a practical point of view, $ r=n $ is unlikely to happen, since we are dealing with a real-life language use where there are more than 1 word type in a text. But for the sake of completing the equation, the following lines were added,\n",
    "```python\n",
    "if sum(VOC(r) for r in range(1, n)) == 0:\n",
    "        return 1\n",
    "```\n",
    "which returns 1 if there is all `r` is zero until $r=n-1$.\n",
    "<br>\n",
    "Simpson's D can be used to compare the query text to candidate texts to find out which candidate text is the closest in terms of lexical diversity. Results will be discussed in section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1807",
   "metadata": {},
   "source": [
    "#### Other lexical stylometric measures\n",
    "##### Mean sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sent(filename): # put raw text, not split by space or deleted punctuation marks!\n",
    "    text = open(filename, 'r').read().replace(\"\\n\",\" \").lower()\n",
    "    t = sent_tokenize(text)\n",
    "    split = []\n",
    "    for sent in t:\n",
    "        a = sent.translate(str.maketrans(\"\",\"\", string.punctuation)).split()\n",
    "        split.append(a)\n",
    "    return sum(len(sent) for sent in split) / len(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892f575",
   "metadata": {},
   "source": [
    "##### Mean word length\n",
    "\n",
    "\n",
    "To make a fairer comparison for mean word length between sample texts, I took the first 15,000 words from each text of `data1`, since the shortest text of data1 (`text_c`) has about 15,000 tokens. The sample texts in `data2` do not have to go through this process, since they all contain more than 50,000 tokens each and there are small differences between the lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a_5k = text_a[:15000]\n",
    "text_b_5k = text_b[:15000]\n",
    "text_c_5k = text_c[:15000]\n",
    "\n",
    "def mean_word(text):\n",
    "    return sum(len(word) for word in text) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df4ad",
   "metadata": {},
   "source": [
    "##### Word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "431e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlength(text):\n",
    "    dist = {}\n",
    "    n = len(max(text, key=len))\n",
    "    X = list(i for i in range(1,n+1))\n",
    "    def length(r):\n",
    "        length = 0\n",
    "        for word in text:\n",
    "            if len(word) == r:\n",
    "                length += 1\n",
    "        return length\n",
    "    for x in X:\n",
    "        dist[x] = length(x)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef016bc",
   "metadata": {},
   "source": [
    "##### Big Words Index\n",
    "refers to the percentage of words with 6 letters or more. <br> In the codes below instead, I used 7 as the threshold, because I was sceptical of this, since 4/5 character long nouns can take plural form and become 6 character long. (Same for present/past tense verbs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BWI(text):\n",
    "    big_word = 0\n",
    "    for word in text:\n",
    "        if len(word) >= 7:\n",
    "            big_word += 1\n",
    "    return round(big_word / len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f5c",
   "metadata": {},
   "source": [
    "##### Lexical Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb1abc",
   "metadata": {},
   "source": [
    "Lexical density is the ratio between the number of lexical items (1-functional words) and the text length.\n",
    "Functional words (stop words) include frequently used words that carry little meaning but grammatical information. Here, I use a predefined list of stop words provided in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def l_density(text):\n",
    "    filtered = []\n",
    "    for w in text:\n",
    "        if w not in stopwords:\n",
    "            filtered.append(w)\n",
    "    return 1 - round(len(filtered) / len(text),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6074bf",
   "metadata": {},
   "source": [
    "\n",
    "Some comparative values were necessary: I followed Savoy(2020, p.30) where it said (and I paraphrase) that an LD value of around 0.3 for an oral production and around 0.4 and higher for writings are the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2965e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd685b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa3a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ec5ddc",
   "metadata": {},
   "source": [
    "#### Distance-based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8095b3",
   "metadata": {},
   "source": [
    "Distance-based methods establish a profile for each candidate author to which we can compare the query text's profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d3b2",
   "metadata": {},
   "source": [
    "Burrow's Delta (Savoy 2020: 34-36) is one of such methods: it considers 40-150 most frequent word types, and the style is reflected through the word choice. According to Savoy(34), 150 most frequent word types cover 50-65% of all tokens in a certain text, with the percentage varying depending on the theme, genre, etc. of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53522b",
   "metadata": {},
   "source": [
    "The following is the formula for Delta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31066087",
   "metadata": {},
   "source": [
    "**(2)** $$Burrow's  Delta(A_{j},Q) = \\displaystyle\\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\mid Zscore(t_{i,A{j}}) - Zscore(t_{i,Q}) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14b2d0",
   "metadata": {},
   "source": [
    "- Aj is a candidate author A's profile.\n",
    "- Q is the query text.\n",
    "- t is a set of word-types in the MFW list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c0911",
   "metadata": {},
   "source": [
    "Each t in the MFW list has the same importance, but the impact depends on their Z score values. <br>\n",
    "To get the Delta value between the query text and a sample text, a list of most frequent word-types is necessary. A relative frequency value for each term can be calculated for each text: the number of occurrences for a certain word-type in a certain text is divided by the length of the text.<br>\n",
    "Then the relative frequency values are compared against each other to get mean and standard deviation values. This is to get Z score for each term in each text: Z score is the relative frequency minus mean divided by standard deviation. Z score helps us understand where a certain value lies in relation to the entire sample. By comparing a Z score for a certain term in both texts, we know how much difference in using that word there is, and the bigger the sum is, the bigger the difference in word choices between the texts will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74134fd6",
   "metadata": {},
   "source": [
    "The function `MFW` below returns a list of 300 most frequent words and their frequency. The number 300 can be changed if necessary. Frequency here is absolute frequency, i.e. how many times it appears in the text.\n",
    "Function `MFW_100` returns the percentage of MFW tokens in relation to the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFW(text):\n",
    "    freq = FreqDist(text)\n",
    "    MFWlist = freq.most_common(300)\n",
    "    return MFWlist\n",
    "\n",
    "def MFW_100(text):\n",
    "    return 100 * sum(i[1] for i in MFW(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b110a",
   "metadata": {},
   "source": [
    "The codes below create a table of most frequent words (MFW) with their absolute frequency in the three respective texts. Obviously, the MFW list is different for each of the text with some overlap, and to be able to compare to each other, I took only MFWs that are present in all three lists, which makes the list shorter than the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b45c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_table(xa, xb, xc):\n",
    "    dict_b = (dict(MFW(xb)))\n",
    "    dict_c = (dict(MFW(xc)))\n",
    "    table = pd.DataFrame(MFW(xa)).rename(columns={0: 'word', 1:'a'})\n",
    "    table.set_index('word',inplace=True)\n",
    "    table[\"b\"] = \"\"\n",
    "    table[\"c\"] = \"\"\n",
    "    for n in MFW(xa):\n",
    "        word = n[0]\n",
    "        if dict_b.get(word) != None and dict_c.get(word) != None:\n",
    "            table.loc[word,\"b\"] = dict_b.get(word)\n",
    "            table.loc[word,\"c\"] = dict_c.get(word)\n",
    "        else:\n",
    "            table.loc[word,\"b\"] = np.nan\n",
    "            table.loc[word,\"c\"] = np.nan\n",
    "        table.dropna(inplace= True)\n",
    "    return table\n",
    "\n",
    "abs_table(text_a,text_b,text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3f0c3",
   "metadata": {},
   "source": [
    "The table we get from `abs_table` is then turned into a relative frequency table. Relative frequency table takes each text's length (number of tokens) into consideration. Since the absolute frequency of MFW will be heavily influenced by the size of the corpus, a relative term frequency is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_table(xa, xb, xc):\n",
    "    table = abs_table(xa, xb, xc)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(xa),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(xb),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(xc),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    return table\n",
    "\n",
    "table = rel_table(text_a,text_b,text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79698bb8",
   "metadata": {},
   "source": [
    "Lastly, the codes below calculate z-score and eventually Delta score. The query text has to be preprocessed before running these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_table(a,b,c):\n",
    "    dict_q = dict(collections.Counter(q))\n",
    "    table = abs_table(a, b, c)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table[\"q\"] = \"\"\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(a),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(b),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(c),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    for word in table[\"words\"]:\n",
    "        if dict_q.get(word) != None:\n",
    "            table.loc[word,\"q\"] = round((dict_q.get(word) / len(q)),5)\n",
    "        else:\n",
    "            table.loc[word,\"q\"] = np.nan\n",
    "    table.loc[:,\"z_a\"] = (table[\"a\"] - table[\"mean\"]) / table[\"sd\"] # calculates z-scores for columns a,b,c,q\n",
    "    table.loc[:,\"z_b\"] = (table[\"b\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_c\"] = (table[\"c\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_q\"] = (table[\"q\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.dropna(inplace= True) # deletes rows that contain NaN\n",
    "    table.drop('words', axis = 'columns',inplace= True) # deletes the redundant column\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(df): # calculates delta score between the column a in the given dataframe and the query text\n",
    "    delta_a = round(sum(list(abs(df[\"z_a\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_b = round(sum(list(abs(df[\"z_b\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_c = round(sum(list(abs(df[\"z_c\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    return delta_a, delta_b, delta_c, 'are Delta distance values between the query text and text a, b, c, respectively.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384cf1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f638797b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f78c637",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9c217",
   "metadata": {},
   "source": [
    "In the previous section, several methods of characterizing writing styles for each author were introduced. In this section, I present results and examine the possibility of attributing correct authors to the query texts based on each index. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9806e",
   "metadata": {},
   "source": [
    "Type-token ratio\n",
    "Simpson's D\n",
    "Mean sentence length\n",
    "Mean word length\n",
    "Word length distribution\n",
    "Big words index\n",
    "Lexical Density\n",
    "Burrow's Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27bc04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81940349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8e36649",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Interpret the results\n",
    "Ways forward (What more could be done, what more am I interested in, what I will do next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fb955",
   "metadata": {},
   "source": [
    "Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd9797",
   "metadata": {},
   "source": [
    "### Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eddb7f",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dea83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
