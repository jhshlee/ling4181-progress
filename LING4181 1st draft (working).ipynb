{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36afeb4f",
   "metadata": {},
   "source": [
    "## Title\n",
    "## : Revisiting and reduplicating authorship attribution methods\n",
    "\n",
    "### LING4181 Supervised Reading\n",
    "### Spring 2023\n",
    "### Jihyeong Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf40dec",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "#### Introduction\n",
    "#### Literature\n",
    "#### Methods\n",
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fefc4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "* What is authorship attribution?\n",
    "* What approaches are there?\n",
    "* What is it useful for?\n",
    "* What was the purpose of me doing this & why reduplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223033f7",
   "metadata": {},
   "source": [
    "### Literature\n",
    "Literature review; summarize important concepts, approaches, methods\n",
    "Recent developments surrounding machine learning models/large language models\n",
    "Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb0ac6",
   "metadata": {},
   "source": [
    "### Methods\n",
    "Explain methods and procedures: present and explain what the codes do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ea901",
   "metadata": {},
   "source": [
    "In this section, I explain in detail the data collection procedure and methods of analysis.\n",
    "As was mentioned in Section 2(Literature), there are various environments to authorship attribution: dichotomous attribution(one candidate), closed-pool attribution(determined number of candidates; say, 3 candidates, one of which the text can be attributed to), infinite-pool attribution(the author can be anyone), etc. In real-world authorship attribution problems, there are also instances of co-authorship, which requires more complex methods of analysis.\n",
    "The methods employed in this essay are applicable for closed-pool, single-author problems, and the data were collected accordingly. The methods can be divided to two large sections: lexical analysis and distance-based analysis. I collected data and wrote the codes myself, but the formula for each quantitative analysis largely followed chapters 2 and 3 from Savoy(2020).\n",
    "\n",
    "I will first briefly explain the datasets and then explain what each method entails and what each block of codes does.\n",
    "\n",
    "#### Datasets\n",
    "Two sets of data consisted with texts from several authors each.\n",
    "As the writing style of an individual varies across themes(what the text is about) and channels(what platform the text was intended for), I decided one channel and two themes: about gardening and true crime, on personal blogs. Sample blogs are found after simple google searches for each keyword \"gardening\" and \"true crime\" and three random blogs each with enough amount of text were selected. In total, texts from 3 gardening-related blogs were included in the first dataset (hereby \"data1\"), and three true-crime blogs were included in the other (\"data2\"). Texts were manually collected by copy-pasting. A short excerpt from each author were stored separately as test(query) text. The query text is not included in the sample text. (In this essay, \"text\" refer to each sample text whose author is the same, unless otherwise noted.)\n",
    "\n",
    "Things worth noting about texts and collecting them:\n",
    "1. I do not know any of the blog owners that were included in the sample. (The source blogs are included in the bibliography.)\n",
    "2. The names of each blog owner were redacted so that the name will not be included in the \"most frequent words\" list\n",
    "3. Author C in data1 was one of three co-writers in the blog, so I filtered out the two other authors and only included one.\n",
    "\n",
    "#### Blog posts as a text\n",
    "EXPLANATION HERE\n",
    "\n",
    "Sample texts as well as full codes can be seen on ((LINK)). Codes are in addition attached as appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e5af9",
   "metadata": {},
   "source": [
    "#### Preparation\n",
    "First, packages as the following are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8006577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "nltk.download('punkt')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e19029",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "Each text went through preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d56e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    text = open(filename, 'r').read().replace(\"\\n\", \" \").lower()\n",
    "    return text.translate(str.maketrans(\"\",\"\", string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eca73a",
   "metadata": {},
   "source": [
    "As such, all letters were converted to lowercase letters, and was removed punctuation marks (more about this later), then split to word units instead of letter unit strings. In other words, all texts after this are a list of all the words it contains, not a string of letters, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fefac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these', 'weeks', 'just', 'after', 'the', 'calendar', 'turns', 'from', 'one', 'year', 'to', 'the', 'next', 'are', 'the', 'perfect', 'time', 'to', 'think', 'about', 'your', 'goals', 'for', 'the', 'coming', 'gardening', 'season', 'on', 'this', 'week’s', 'podcast', 'i', 'discuss', 'plotting', 'out', 'plans', 'for', 'doubling', 'down', 'on', 'what', 'worked', 'well', 'in', 'the', 'garden', 'while', 'also', 'deciding', 'on', 'what', 'i', 'want', 'to', 'stop', 'doing', 'and', 'identifying', 'new', 'things', 'that', 'i’d', 'like', 'to', 'give', 'a', 'try', 'last', 'week', 'i', 'discussed', 'my', '10', 'garden', 'lessons', 'from', '2022', 'and', 'now', 'i', 'am', 'shifting', 'gears', 'from', 'looking', 'back', 'to', 'forging', 'ahead', 'there', 'are', 'things', 'that', 'i', 'have', 'experimented', 'with', 'in', 'recent', 'years', 'to', 'varying', 'degrees', 'of', 'success', 'and', 'i', 'want', 'to', 'take', 'those', 'lessons', 'and', 'move', 'forward', 'refining', 'and', 'enhancing', 'the', 'methods', 'that', 'worked', 'for', 'me', 'i', 'always', 'want', 'to', 'continue', 'spending', 'more', 'time', 'in', 'the', 'garden', 'just', 'observing', 'and', 'monitoring', 'which', 'is', 'something', 'i', 'had', 'amazing', 'success', 'with', 'last', 'year', 'name', 'with', 'just', 'harvested', 'onions', 'there', 'is', 'so', 'much', 'i', 'am', 'looking', 'forward', 'to', 'doing', 'in', '2023', 'including', 'repeating', 'and', 'doubling', 'down', 'on', 'gardening', 'practices', 'that', 'were', 'great', 'successes', 'to', 'trying', 'new', 'and', 'promising', 'methods', 'before', 'getting', 'into', 'it', 'i', 'want', 'to', 'take', 'a', 'second', 'to', 'remind', 'you', 'that', 'i', 'have']\n"
     ]
    }
   ],
   "source": [
    "text_a = preprocess('author_a.txt')\n",
    "text_b = preprocess('author_b.txt')\n",
    "text_c = preprocess('author_c.txt')\n",
    "\n",
    "print(text_a[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788064a0",
   "metadata": {},
   "source": [
    "#### Basic data summarization\n",
    "Each text has more than 15,000 words(tokens), with the smallest being the \"author C\" in data1.\n",
    "\n",
    "##### Tokens, types, type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdcdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens\n",
    "\n",
    "print(len(text_a))\n",
    "print(len(text_b))\n",
    "print(len(text_c))\n",
    "\n",
    "# number of types\n",
    "\n",
    "print(len(set(text_a)))\n",
    "print(len(set(text_b)))\n",
    "print(len(set(text_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40436db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이 부분은 각 데이터셋별로 하나의 테이블로 나타내서 총 2개의 테이블로 visualize하면 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4e13a",
   "metadata": {},
   "source": [
    "Obviously, the bigger the text is, the better. The smallest text was ((()))) from ((((dataset)))) with ((())) types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cba1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터셋이 더 크면 더 정확도가 높아지는지 실험하면 재밌겠다 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b890e",
   "metadata": {},
   "source": [
    "#### Lexical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa913d2",
   "metadata": {},
   "source": [
    "Lexical analysis methods refer to methods that make use of surface lexical information. There have been suggested several such methods that utilize different aspects of the usage of words.\n",
    "##### Type-token ratio\n",
    "Type-token ratio is a simple index for measuring lexical diversity in a given text. A low type-token ratio indicates low degree of diversity in the choice of words the author made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c23b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1644\n",
      "0.159\n",
      "0.1501\n"
     ]
    }
   ],
   "source": [
    "def typetoken_ratio(text):\n",
    "    return round(len(set(text))/len(text),4)\n",
    "\n",
    "print(typetoken_ratio(text_a))\n",
    "print(typetoken_ratio(text_b))\n",
    "print(typetoken_ratio(text_c))\n",
    "# Visualize as a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ae6dc",
   "metadata": {},
   "source": [
    "# 이 밑에 Type-token ratio 계산 부분은 나중에 result 논의할 때 적는 게 낫겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fda01a",
   "metadata": {},
   "source": [
    "To compare to these, we need type-token ratio for the query text as well. As an example, one of the query texts are shown below: a more detailed results will be discussed in Section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69a7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2593\n"
     ]
    }
   ],
   "source": [
    "q1 = preprocess('q1.txt')\n",
    "print(lexical_diversity(q1))\n",
    "\n",
    "## 여기에 차의 절댓값을 사용해서 가장 가까운 수 (차의 절댓값이 가장 작은 수) 찾는 함수를 만들면 재밌겠다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5365ff7",
   "metadata": {},
   "source": [
    "Type-token ratio for `q1` is the closest to `text_a`, which0 is an accurate attribution result as `q1` is indeed written by Author A. The method seems to work, but since the difference between the type-token ratio of the query text and the closest sample text is bigger than that between the sample texts, it is doubtful how effective this method is in the grand scheme.\n",
    "This is partially due to the fact that the query text is short (((HOW MANY WORDS))) compared to the query texts. The type-token ratio is heavily influenced by the length of the text. Text length can be expanded infinitely, but the use of vocabulary does not follow the same rate, since the most frequent words take up the majority of the words we use, as we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe2ad",
   "metadata": {},
   "source": [
    "#### Simpson's D\n",
    "Among other ways to measure lexical diversity that were introduced in Savoy(2020), Simpson's D(1949) is a useful index that is less influenced by the text length. Simpson's D measures vocabulary richness by calculating the sum of probabilities \n",
    "of selecting the same word twice in two separate trials. The formula is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b60ac",
   "metadata": {},
   "source": [
    "**(1)** $$Simpson's  D(T) = \\displaystyle\\sum_{r=1} \\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot \\mid Voc_{r}(T) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583330e4",
   "metadata": {},
   "source": [
    "- T refers to the text.\n",
    "- n refers to the corpus size, i.e. the number of tokens in T.\n",
    "- r refers to the number of times a given word type appears in T.\n",
    "- VOC<sub>r</sub>(T) refers to the number of word types that appear in T exactly r times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fd62",
   "metadata": {},
   "source": [
    "When there is no diversity in the usage of words, in other words there are only one word that is used throughout the entire text ($ r=n $), the formula returns 1, which is the maximum value, Hence, the closer to 0 Simpson's D value is, the richer the vocabulary use is for the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7c7ba",
   "metadata": {},
   "source": [
    "The formula in **(1)** is rewritten as codes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson_D(text):\n",
    "    count = collections.Counter(text)\n",
    "    types = set(text)\n",
    "    n = len(text)\n",
    "    def VOC(r):\n",
    "        VOC = 0\n",
    "        for i in types: # i is a word(type)\n",
    "            if count.get(i) == r:\n",
    "                VOC += 1\n",
    "        return VOC\n",
    "    if sum(VOC(r) for r in range(1, n-1)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(sum(VOC(r) * (r**2 - r) / (n**2 - n) for r in range(1,n)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261e2d",
   "metadata": {},
   "source": [
    "The function `simpson_D(text)` takes a preprocessed text as its argument. `collections.Counter` function creates a dictionary type data where the key is the word type and the value is its occurence in the text. `VOC(r)` is an inner function that is defined as the number of word types(`i`) in `text` that appears r times. Since it presupposes `i` appears at least once in `text`, `VOC(r)` always appears as a positive number. Hence the absolute value sign in **(1)** is unnecessary. The function, then, calculates the sum of $\\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot Voc_{r}$ for all $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f771c",
   "metadata": {},
   "source": [
    "By  definition, the function should return 1 when $r=n$. However, the code above somehow always returns `0.0`. From a practical point of view, $ r=n $ is unlikely to happen, since we are dealing with a real-life language use where there are more than 1 word type in a text. But for the sake of completing the equation, the following lines were added,\n",
    "```python\n",
    "if sum(VOC(r) for r in range(1, n-1)) == 0:\n",
    "        return 1\n",
    "```\n",
    "which returns 1 if there is all `r` is zero until $r=n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1807",
   "metadata": {},
   "source": [
    "#### Mean word length\n",
    "\n",
    "To make a fairer comparison for mean word length between sample texts, I took the first 15,000 words from each text of data1, since the shortest text of data1 (`text_c`) has about 15,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a_5k = text_a[:15000]\n",
    "text_b_5k = text_b[:15000]\n",
    "text_c_5k = text_c[:15000]\n",
    "\n",
    "def average(text):\n",
    "    return sum(len(word) for word in text) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df4ad",
   "metadata": {},
   "source": [
    "#### Word length distribution\n",
    "# WRITE THE CODES #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f3b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f5c",
   "metadata": {},
   "source": [
    "#### Big word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa59396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfbf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dabb1203",
   "metadata": {},
   "source": [
    "Lexical density: function words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa3a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ec5ddc",
   "metadata": {},
   "source": [
    "#### Distance-based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d3b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f78c637",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e36649",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Interpret the results\n",
    "Ways forward (What more could be done, what more am I interested in, what I will do next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbb53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
