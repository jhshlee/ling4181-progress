{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36afeb4f",
   "metadata": {},
   "source": [
    "## Title\n",
    "## : Revisiting and reduplicating authorship attribution methods\n",
    "\n",
    "### LING4181 Supervised Reading\n",
    "### Spring 2023\n",
    "### Jihyeong Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf40dec",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "#### Introduction\n",
    "#### Literature\n",
    "#### Methods\n",
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fefc4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "* What is authorship attribution?\n",
    "* What approaches are there?\n",
    "* What is it useful for?\n",
    "* What was the purpose of me doing this & why reduplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223033f7",
   "metadata": {},
   "source": [
    "### Literature\n",
    "Literature review; summarize important concepts, approaches, methods\n",
    "Recent developments surrounding machine learning models/large language models\n",
    "Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb0ac6",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ea901",
   "metadata": {},
   "source": [
    "In this section, I explain in detail the data collection procedure and methods of analysis.\n",
    "As was mentioned in Section 2(Literature), there are various environments to authorship attribution: dichotomous attribution(one candidate), closed-pool attribution(determined number of candidates; say, 3 candidates, one of which the text can be attributed to), infinite-pool attribution(the author can be anyone), etc. In real-world authorship attribution problems, there are also instances of co-authorship, which requires more complex methods of analysis.\n",
    "The methods employed in this essay are applicable for closed-pool, single-author problems, and the data were collected accordingly. The methods can be divided to two large sections: lexical analysis and distance-based analysis. I collected data and wrote the codes myself, but the formula for each quantitative analysis largely followed chapters 2 and 3 from Savoy(2020).\n",
    "\n",
    "I will first briefly explain the datasets and then explain what each method entails and what each block of codes does.\n",
    "\n",
    "#### Datasets\n",
    "Two sets of data consisted with texts from several authors each.\n",
    "As the writing style of an individual varies across themes(what the text is about) and channels(what platform the text was intended for), I decided one channel and two themes: about gardening and true crime, on personal blogs. Sample blogs are found after simple google searches for each keyword \"gardening\" and \"true crime\" and three random blogs each with enough amount of text were selected. In total, texts from 3 gardening-related blogs were included in the first dataset (hereby \"data1\"), and three true-crime blogs were included in the other (\"data2\"). Texts were manually collected by copy-pasting. A short excerpt from each author were stored separately as test(query) text. The query text is not included in the sample text. (In this essay, \"text\" refer to each sample text whose author is the same, unless otherwise noted.)\n",
    "\n",
    "Things worth noting about texts and collecting them:\n",
    "1. I do not know any of the blog owners that were included in the sample. (Links to the source blogs are included in the bibliography.)\n",
    "2. Author C in data1 was one of three co-writers in the blog, so I filtered out the two other authors and only included one.\n",
    "\n",
    "#### Blog posts as a text\n",
    "EXPLANATION HERE\n",
    "\n",
    "Sample texts as well as full codes can be seen on ((LINK)). Codes are in addition attached as appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e5af9",
   "metadata": {},
   "source": [
    "#### Preparation\n",
    "First, packages as the following are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8006577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "nltk.download('punkt')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e19029",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "Each text went through preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d56e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    text = open(filename, 'r').read().replace(\"\\n\", \" \").lower()\n",
    "    return text.translate(str.maketrans(\"\",\"\", string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eca73a",
   "metadata": {},
   "source": [
    "As such, all letters were converted to lowercase letters, and was removed punctuation marks (more about this later), then split to word units instead of letter unit strings. In other words, all texts after this are a list of all the words it contains, not a string of letters, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fefac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these', 'weeks', 'just', 'after', 'the', 'calendar', 'turns', 'from', 'one', 'year', 'to', 'the', 'next', 'are', 'the', 'perfect', 'time', 'to', 'think', 'about', 'your', 'goals', 'for', 'the', 'coming', 'gardening', 'season', 'on', 'this', 'week’s', 'podcast', 'i', 'discuss', 'plotting', 'out', 'plans', 'for', 'doubling', 'down', 'on', 'what', 'worked', 'well', 'in', 'the', 'garden', 'while', 'also', 'deciding', 'on', 'what', 'i', 'want', 'to', 'stop', 'doing', 'and', 'identifying', 'new', 'things', 'that', 'i’d', 'like', 'to', 'give', 'a', 'try', 'last', 'week', 'i', 'discussed', 'my', '10', 'garden', 'lessons', 'from', '2022', 'and', 'now', 'i', 'am', 'shifting', 'gears', 'from', 'looking', 'back', 'to', 'forging', 'ahead', 'there', 'are', 'things', 'that', 'i', 'have', 'experimented', 'with', 'in', 'recent', 'years', 'to', 'varying', 'degrees', 'of', 'success', 'and', 'i', 'want', 'to', 'take', 'those', 'lessons', 'and', 'move', 'forward', 'refining', 'and', 'enhancing', 'the', 'methods', 'that', 'worked', 'for', 'me', 'i', 'always', 'want', 'to', 'continue', 'spending', 'more', 'time', 'in', 'the', 'garden', 'just', 'observing', 'and', 'monitoring', 'which', 'is', 'something', 'i', 'had', 'amazing', 'success', 'with', 'last', 'year', 'name', 'with', 'just', 'harvested', 'onions', 'there', 'is', 'so', 'much', 'i', 'am', 'looking', 'forward', 'to', 'doing', 'in', '2023', 'including', 'repeating', 'and', 'doubling', 'down', 'on', 'gardening', 'practices', 'that', 'were', 'great', 'successes', 'to', 'trying', 'new', 'and', 'promising', 'methods', 'before', 'getting', 'into', 'it', 'i', 'want', 'to', 'take', 'a', 'second', 'to', 'remind', 'you', 'that', 'i', 'have']\n"
     ]
    }
   ],
   "source": [
    "text_a = preprocess('author_a.txt')\n",
    "text_b = preprocess('author_b.txt')\n",
    "text_c = preprocess('author_c.txt')\n",
    "\n",
    "print(text_a[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788064a0",
   "metadata": {},
   "source": [
    "#### Basic data summarization\n",
    "Each text has more than 15,000 words(tokens), with the smallest being the \"author C\" in data1.\n",
    "\n",
    "##### Tokens, types, type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdcdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens\n",
    "\n",
    "print(len(text_a))\n",
    "print(len(text_b))\n",
    "print(len(text_c))\n",
    "\n",
    "# number of types\n",
    "\n",
    "print(len(set(text_a)))\n",
    "print(len(set(text_b)))\n",
    "print(len(set(text_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b890e",
   "metadata": {},
   "source": [
    "#### Lexical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa913d2",
   "metadata": {},
   "source": [
    "Lexical analysis methods refer to methods that make use of surface lexical information. There have been suggested several such methods that utilize different aspects of the usage of words.\n",
    "##### Type-token ratio\n",
    "Type-token ratio is a simple index for measuring lexical diversity in a given text. A low type-token ratio indicates low degree of diversity in the choice of words the author made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c23b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1644\n",
      "0.159\n",
      "0.1501\n"
     ]
    }
   ],
   "source": [
    "def typetoken_ratio(text):\n",
    "    return round(len(set(text))/len(text),4)\n",
    "\n",
    "print(typetoken_ratio(text_a))\n",
    "print(typetoken_ratio(text_b))\n",
    "print(typetoken_ratio(text_c))\n",
    "# Visualize as a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ae6dc",
   "metadata": {},
   "source": [
    "# 이 밑에 Type-token ratio 계산 부분은 나중에 result 논의할 때 적는 게 낫겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fda01a",
   "metadata": {},
   "source": [
    "To compare to these, we need type-token ratio for the query text as well. As an example, one of the query texts are shown below: a more detailed results will be discussed in Section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69a7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2593\n"
     ]
    }
   ],
   "source": [
    "q1 = preprocess('q1.txt')\n",
    "print(lexical_diversity(q1))\n",
    "\n",
    "## 여기에 차의 절댓값을 사용해서 가장 가까운 수 (차의 절댓값이 가장 작은 수) 찾는 함수를 만들면 재밌겠다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5365ff7",
   "metadata": {},
   "source": [
    "Type-token ratio for `q1` is the closest to `text_a`, which0 is an accurate attribution result as `q1` is indeed written by Author A. The method seems to work, but since the difference between the type-token ratio of the query text and the closest sample text is bigger than that between the sample texts, it is doubtful how effective this method is in the grand scheme.\n",
    "This is partially due to the fact that the query text is short (((HOW MANY WORDS))) compared to the query texts. The type-token ratio is heavily influenced by the length of the text. Text length can be expanded infinitely, but the use of vocabulary does not follow the same rate, since the most frequent words take up the majority of the words we use, as we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe2ad",
   "metadata": {},
   "source": [
    "#### Simpson's D\n",
    "Among other ways to measure lexical diversity that were introduced in Savoy(2020), Simpson's D(1949) is a useful index that is less influenced by the text length. Simpson's D measures vocabulary richness by calculating the sum of probabilities \n",
    "of selecting the same word twice in two separate trials. The formula is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b60ac",
   "metadata": {},
   "source": [
    "**(1)** $$Simpson's  D(T) = \\displaystyle\\sum_{r=1} \\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot \\mid Voc_{r}(T) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583330e4",
   "metadata": {},
   "source": [
    "- T refers to the text.\n",
    "- n refers to the corpus size, i.e. the number of tokens in T.\n",
    "- r refers to the number of times a given word type appears in T.\n",
    "- VOC<sub>r</sub>(T) refers to the number of word types that appear in T exactly r times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fd62",
   "metadata": {},
   "source": [
    "When there is no diversity in the usage of words, in other words there are only one word that is used throughout the entire text ($ r=n $), the formula returns 1, which is the maximum value, Hence, the closer to 0 Simpson's D value is, the richer the vocabulary use is for the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7c7ba",
   "metadata": {},
   "source": [
    "The formula in **(1)** is rewritten as codes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function calculates Simpson_D index for a given text\n",
    "def simpson_D(text):\n",
    "    count = collections.Counter(text)\n",
    "    types = set(text)\n",
    "    n = len(text)\n",
    "    def VOC(r):\n",
    "        VOC = 0\n",
    "        for i in types: # i is a word(type)\n",
    "            if count.get(i) == r:\n",
    "                VOC += 1\n",
    "        return VOC\n",
    "    if sum(VOC(r) for r in range(1, n-1)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(sum(VOC(r) * (r**2 - r) / (n**2 - n) for r in range(1,n)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261e2d",
   "metadata": {},
   "source": [
    "The function `simpson_D(text)` takes a preprocessed text as its argument. `collections.Counter` function creates a dictionary type data where the key is the word type and the value is its occurence in the text. `VOC(r)` is an inner function that is defined as the number of word types(`i`) in `text` that appears r times. Since it presupposes `i` appears at least once in `text`, `VOC(r)` always appears as a positive number. Hence the absolute value sign in **(1)** is unnecessary. The function, then, calculates the sum of $\\frac{r}{n} \\cdot \\frac{r-1}{n-1} \\cdot Voc_{r}$ for all $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f771c",
   "metadata": {},
   "source": [
    "By  definition, the function should return 1 when $r=n$. However, the code above somehow always returns `0.0`. From a practical point of view, $ r=n $ is unlikely to happen, since we are dealing with a real-life language use where there are more than 1 word type in a text. But for the sake of completing the equation, the following lines were added,\n",
    "```python\n",
    "if sum(VOC(r) for r in range(1, n-1)) == 0:\n",
    "        return 1\n",
    "```\n",
    "which returns 1 if there is all `r` is zero until $r=n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1807",
   "metadata": {},
   "source": [
    "#### Mean word length\n",
    "\n",
    "To make a fairer comparison for mean word length between sample texts, I took the first 15,000 words from each text of data1, since the shortest text of data1 (`text_c`) has about 15,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a_5k = text_a[:15000]\n",
    "text_b_5k = text_b[:15000]\n",
    "text_c_5k = text_c[:15000]\n",
    "\n",
    "def average(text):\n",
    "    return sum(len(word) for word in text) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df4ad",
   "metadata": {},
   "source": [
    "#### Word length distribution\n",
    "# WRITE THE CODES #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f3b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f5c",
   "metadata": {},
   "source": [
    "#### Lexical Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb1abc",
   "metadata": {},
   "source": [
    "Lexical density is the ratio between the number of lexical items (1-functional words) and the text length.\n",
    "Functional words include determiners, pronouns, prepositions, auxiliary verbs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfbf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dabb1203",
   "metadata": {},
   "source": [
    "Lexical density: function words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa3a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ec5ddc",
   "metadata": {},
   "source": [
    "#### Distance-based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8095b3",
   "metadata": {},
   "source": [
    "Distance-based methods establish a profile for each candidate author to which we can compare the query text's profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d3b2",
   "metadata": {},
   "source": [
    "Burrow's Delta (Savoy 2020: 34-36) is one of such methods: it considers 40-150 most frequent word types, and the style is reflected through the word choice. According to Savoy(34), 150 most frequent word types cover 50-65% of all tokens in a certain text, with the percentage varying depending on the theme, genre, etc. of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53522b",
   "metadata": {},
   "source": [
    "The following is the formula for Delta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31066087",
   "metadata": {},
   "source": [
    "**(2)** $$Burrow's  Delta(A_{j},Q) = \\displaystyle\\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\mid Zscore(t_{i,A{j}}) - Zscore(t_{i,Q}) \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14b2d0",
   "metadata": {},
   "source": [
    "- Aj is a candidate author A's profile.\n",
    "- Q is the query text.\n",
    "- t is a set of word-types in the MFW list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c0911",
   "metadata": {},
   "source": [
    "Each t in the MFW list has the same importance, but the impact depends on their Z score values. <br>\n",
    "To get the Delta value between the query text and a sample text, a list of most frequent word-types is necessary. A relative frequency value for each term can be calculated for each text: the number of occurrences for a certain word-type in a certain text is divided by the length of the text.<br>\n",
    "Then the relative frequency values are compared against each other to get mean and standard deviation values. This is to get Z score for each term in each text: Z score is the relative frequency minus mean divided by standard deviation. Z score helps us understand where a certain value lies in relation to the entire sample. By comparing a Z score for a certain term in both texts, we know how much difference in using that word there is, and the bigger the sum is, the bigger the difference in word choices between the texts will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74134fd6",
   "metadata": {},
   "source": [
    "The function `MFW` below returns a list of 300 most frequent words and their frequency. The number 300 can be changed if necessary. Frequency here is absolute frequency, i.e. how many times it appears in the text.\n",
    "Function `MFW_100` returns the percentage of MFW tokens in relation to the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFW(text):\n",
    "    freq = FreqDist(text)\n",
    "    MFWlist = freq.most_common(300)\n",
    "    return MFWlist\n",
    "\n",
    "def MFW_100(text):\n",
    "    return 100 * sum(i[1] for i in MFW(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b110a",
   "metadata": {},
   "source": [
    "The codes below create a table of most frequent words (MFW) with their absolute frequency in the three respective texts. Obviously, the MFW list is different for each of the text with some overlap, and to be able to compare to each other, I took only MFWs that are present in all three lists, which makes the list shorter than the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b45c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_table(xa, xb, xc):\n",
    "    dict_b = (dict(MFW(xb)))\n",
    "    dict_c = (dict(MFW(xc)))\n",
    "    table = pd.DataFrame(MFW(xa)).rename(columns={0: 'word', 1:'a'})\n",
    "    table.set_index('word',inplace=True)\n",
    "    table[\"b\"] = \"\"\n",
    "    table[\"c\"] = \"\"\n",
    "    for n in MFW(xa):\n",
    "        word = n[0]\n",
    "        if dict_b.get(word) != None and dict_c.get(word) != None:\n",
    "            table.loc[word,\"b\"] = dict_b.get(word)\n",
    "            table.loc[word,\"c\"] = dict_c.get(word)\n",
    "        else:\n",
    "            table.loc[word,\"b\"] = np.nan\n",
    "            table.loc[word,\"c\"] = np.nan\n",
    "        table.dropna(inplace= True)\n",
    "    return table\n",
    "\n",
    "abs_table(text_a,text_b,text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3f0c3",
   "metadata": {},
   "source": [
    "The table we get from `abs_table` is then turned into a relative frequency table. Relative frequency table takes each text's length (number of tokens) into consideration. Since the absolute frequency of MFW will be heavily influenced by the size of the corpus, a relative term frequency is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_table(xa, xb, xc):\n",
    "    table = abs_table(xa, xb, xc)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(xa),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(xb),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(xc),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    return table\n",
    "\n",
    "table = rel_table(text_a,text_b,text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79698bb8",
   "metadata": {},
   "source": [
    "Lastly, the codes below calculate z-score and eventually Delta score. The query text has to be preprocessed before running these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = preprocess('q3.txt')\n",
    "\n",
    "def zscore_table(a,b,c):\n",
    "    dict_q = dict(collections.Counter(q))\n",
    "    table = abs_table(a, b, c)\n",
    "    table = table.astype(float)\n",
    "    table[\"words\"] = table.index\n",
    "    table[\"q\"] = \"\"\n",
    "    table.loc[:,\"a\"] = round(table[\"a\"] / len(a),5)\n",
    "    table.loc[:,\"b\"] = round(table[\"b\"] / len(b),5)\n",
    "    table.loc[:,\"c\"] = round(table[\"c\"] / len(c),5)\n",
    "    table.loc[:,\"mean\"] = table.mean(axis='columns')\n",
    "    table.loc[:,\"sd\"] = table.std(axis='columns')\n",
    "    for word in table[\"words\"]:\n",
    "        if dict_q.get(word) != None:\n",
    "            table.loc[word,\"q\"] = round((dict_q.get(word) / len(q)),5)\n",
    "        else:\n",
    "            table.loc[word,\"q\"] = np.nan\n",
    "    table.loc[:,\"z_a\"] = (table[\"a\"] - table[\"mean\"]) / table[\"sd\"] # calculates z-scores for columns a,b,c,q\n",
    "    table.loc[:,\"z_b\"] = (table[\"b\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_c\"] = (table[\"c\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.loc[:,\"z_q\"] = (table[\"q\"] - table[\"mean\"]) / table[\"sd\"]\n",
    "    table.dropna(inplace= True) # deletes rows that contain NaN\n",
    "    table.drop('words', axis = 'columns',inplace= True) # deletes the redundant column\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(df): # calculates delta score between the column a in the given dataframe and the query text\n",
    "    delta_a = round(sum(list(abs(df[\"z_a\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_b = round(sum(list(abs(df[\"z_b\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    delta_c = round(sum(list(abs(df[\"z_c\"]-df[\"z_q\"]))) / len(df),5)\n",
    "    return delta_a, delta_b, delta_c, 'are Delta distance values between the query text and text a, b, c, respectively.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384cf1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029c937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6feb310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f78c637",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9806e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22c9c217",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf27bc04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81940349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fae4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8e36649",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Interpret the results\n",
    "Ways forward (What more could be done, what more am I interested in, what I will do next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbb53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
